{"./":{"url":"./","title":"简介","keywords":"","body":"Web开发者中心-文档 Web 技术发展至今，已经细分成了多个技术方向： Web 整体设计 Web 性能速度 Web 安全 Web App Web 能力增强 学习使用最新的 Web 技术，创建极速打开、信息安全、交互流畅的网页。 "},"web-tech/fis.html":{"url":"web-tech/fis.html","title":"使用 FIS","keywords":"","body":"使用 FIS 官网：http://fis.baidu.com/fis3/ FIS3 是什么？ FIS3 是面向前端的工程构建工具。解决前端工程中性能优化、资源加载（异步、同步、按需、预加载、依赖管理、合并、内嵌）、模块化开发、自动化工具、开发规范、代码部署等问题。 安装 fis3-安装指南：http://fis.baidu.com/fis3/docs/beginning/install.html 使用教程 fis3-构建方式：http://fis.baidu.com/fis3/docs/beginning/release.html 技术交流 github: fex-team/fis3 "},"web-tech/use-mip-to-create-mobile-web-pages.html":{"url":"web-tech/use-mip-to-create-mobile-web-pages.html","title":"MIP-移动页面加速器","keywords":"","body":"使用MIP技术 创建秒开的移动端页面 官网：https://www.mipengine.org/ 什么是 MIP MIP （Mobile Instant Pages - 移动网页加速器）, 是一套应用于移动网页的开放性技术标准。通过提供 MIP-HTML 规范、MIP-JS 运行环境以及 MIP-Cache 页面缓存系统，实现移动网页加速。 MIP 主要由三部分组织成： MIP HTML MIP JS MIP Cache MIP HTML 基于 HTML 中的基础标签制定了全新的规范，通过对一部分基础标签的使用限制或功能扩展，使 HTML 能够展现更加丰富的内容；MIP JS 可以保证 MIP HTML 页面的快速渲染；MIP Cache 用于实现 MIP 页面的高速缓存，从而进一步提高页面性能。 详见 MIP官网：什么是 MIP MIP 加速原理 页面渲染耗时主要分为以下三类：DNS解析耗时，网络传输耗时，浏览器渲染重绘耗时。 MIP加速的基本原理就是减少各个步骤的耗时： 为减少DNS解析，MIP-Cache将静态文件以相对路径储存在百度CDN中； 为减少网络传输耗时，MIP-JS 控制静态资源的按需加载，MIP-Cache系统优先从CDN读取文件； 为减少浏览器渲染重绘耗时，MIP-HTML 对,等造成浏览器重绘的标签进行了封装控制。 详见 MIP博客：百度MIP移动页面加速——不只是CDN MIP 开发入门 MIP HTML 基于 HTML 基础规范进行了扩展，在 HTML 的基础上禁用了一些会影响性能的标签。 开发文档请见：MIP 新手指南MIP 视频教程： MIP 系列视频教程(共五集) MIP 技术交流 github: mipengine/mip-extensionsQQ 群：580967494 "},"web-tech/lavas-pwa.html":{"url":"web-tech/lavas-pwa.html","title":"Lavas PWA-渐进式应用","keywords":"","body":"使用Lavas PWA创建移动应用 官网：https://pwa.baidu.com/ PWA 是什么 Progressive Web App, 简称 PWA，是提升 Web App 的体验的一种新方法，能给用户原生应用的体验。 PWA 能做到原生应用的体验不是靠特指某一项技术，而是经过应用一些新技术进行改进，在安全、性能和体验三个方面都有很大提升，PWA 本质上是 Web App，借助一些新技术也具备了 Native App 的一些特性，兼具 Web App 和 Native App 的优点。 PWA 的主要特点包括下面三点： 可靠 - 即使在不稳定的网络环境下，也能瞬间加载并展现 体验 - 快速响应，并且有平滑的动画响应用户的操作 粘性 - 像设备上的原生应用，具有沉浸式的用户体验，用户可以添加到桌面 详细介绍请见 Lavas 官网：什么是 PWA Lavas 是什么 Lavas 是一个基于 Vue 的 PWA (Progressive Web Apps) 完整解决方案。我们将 PWA 的工程实践总结成多种 Lavas 应用框架模板，帮助开发者轻松搭建 PWA 站点，且无需过多的关注 PWA 开发本身。 详细介绍请见 Lavas 官网：Lavas 介绍 如何开发 具体步骤请见 Lavas 官网：快速开始 PWA 工程 技术交流 github: lavas-projectQQ 群：655433298 "},"design/placeholder.html":{"url":"design/placeholder.html","title":"响应式布局","keywords":"","body":"内容空 作者：Jenny_L 安装 这里是安装方法 试用一下 快来试用. // 这里是代码 var a = [3,4,5]; console.log(a[0]); 更多阅读 "},"security/https-pratice-1.html":{"url":"security/https-pratice-1.html","title":"大型网站的HTTPS实践（一）-- HTTPS协议和原理","keywords":"","body":"大型网站的 HTTPS 实践（一）-- HTTPS 协议和原理 前言 百度已经于近日上线了全站 HTTPS 的安全搜索，默认会将 HTTP 请求跳转成 HTTPS。本文重点介绍 HTTPS 协议 ,并简单介绍部署全站 HTTPS 的意义。 HTTPS 协议概述 HTTPS 可以认为是 HTTP + TLS。HTTP 协议大家耳熟能详了，目前大部分 WEB 应用和网站都是使用 HTTP 协议传输的。TLS 是传输层加密协议，它的前身是 SSL 协议，最早由 netscape 公司于 1995 年发布，1999 年经过 IETF 讨论和规范后，改名为 TLS。如果没有特别说明，SSL 和 TLS 说的都是同一个协议。HTTP 和 TLS 在协议层的位置以及 TLS 协议的组成如下图： TLS 协议主要有五部分：应用数据层协议，握手协议，报警协议，加密消息确认协议，心跳协议。TLS 协议本身又是由 record 协议传输的，record 协议的格式如上图最右所示。 目前常用的 HTTP 协议是 HTTP1.1，常用的 TLS 协议版本有如下几个：TLS1.2, TLS1.1, TLS1.0 和 SSL3.0。其中 SSL3.0 由于 POODLE 攻击已经被证明不安全，但统计发现依然有不到 1% 的浏览器使用 SSL3.0。TLS1.0 也存在部分安全漏洞，比如 RC4 和 BEAST 攻击。TLS1.2 和 TLS1.1 暂时没有已知的安全漏洞，比较安全，同时有大量扩展提升速度和性能，推荐大家使用。需要关注一点的就是 TLS1.3 将会是 TLS 协议一个非常重大的改革。不管是安全性还是用户访问速度都会有质的提升。不过目前没有明确的发布时间。 HTTPS 功能介绍 百度使用 HTTPS 协议主要是为了保护用户隐私，防止流量劫持。HTTP 本身是明文传输的，没有经过任何安全处理。例如用户在百度搜索了一个关键字，比如 “苹果手机”，中间者完全能够查看到这个信息，并且有可能打电话过来骚扰用户。也有一些用户投诉使用百度时，发现首页或者结果页面浮了一个很长很大的广告，这也肯定是中间者往页面插的广告内容。如果劫持技术比较低劣的话，用户甚至无法访问百度。这里提到的中间者主要指一些网络节点，是用户数据在浏览器和百度服务器中间传输必须要经过的节点。比如 WIFI 热点，路由器，防火墙，反向代理，缓存服务器等。在 HTTP 协议下，中间者可以随意嗅探用户搜索内容，窃取隐私甚至篡改网页。不过 HTTPS 是这些劫持行为的克星，能够完全有效地防御。总体来说，HTTPS 协议提供了三个强大的功能来对抗上述的劫持行为： 内容加密。浏览器到百度服务器的内容都是以加密形式传输，中间者无法直接查看原始内容。 身份认证。保证用户访问的是百度服务，即使被 DNS 劫持到了第三方站点，也会提醒用户没有访问百度服务，有可能被劫持 数据完整性。防止内容被第三方冒充或者篡改。 那 HTTPS 是如何做到上述三点的呢？下面从原理角度介绍一下。 HTTPS 原理介绍 1. 内容加密 加密算法一般分为两种，对称加密和非对称加密。所谓对称加密（也叫密钥加密）就是指加密和解密使用的是相同的密钥。而非对称加密（也叫公钥加密）就是指加密和解密使用了不同的密钥。 非对称密钥交换 在非对称密钥交换算法出现以前，对称加密一个很大的问题就是不知道如何安全生成和保管密钥。非对称密钥交换过程主要就是为了解决这个问题，使得对称密钥的生成和使用更加安全。 密钥交换算法本身非常复杂，密钥交换过程涉及到随机数生成，模指数运算，空白补齐，加密，签名等操作。 常见的密钥交换算法有 RSA，ECDHE，DH，DHE 等算法。它们的特性如下： RSA：算法实现简单，诞生于 1977 年，历史悠久，经过了长时间的破解测试，安全性高。缺点就是需要比较大的素数（目前常用的是 2048 位）来保证安全强度，很消耗 CPU 运算资源。RSA 是目前唯一一个既能用于密钥交换又能用于证书签名的算法。 DH：diffie-hellman 密钥交换算法，诞生时间比较早（1977 年），但是 1999 年才公开。缺点是比较消耗 CPU 性能。 ECDHE：使用椭圆曲线（ECC）的 DH 算法，优点是能用较小的素数（256 位）实现 RSA 相同的安全等级。缺点是算法实现复杂，用于密钥交换的历史不长，没有经过长时间的安全攻击测试。 ECDH：不支持 PFS，安全性低，同时无法实现 false start。 DHE：不支持 ECC。非常消耗性能。 百度只支持 RSA 和 ECDH_RSA 密钥交换算法。原因是： ECDHE 支持 ECC 加速，计算速度更快。支持 PFS，更加安全。支持 false start，用户访问速度更快。 目前还有至少 20% 以上的客户端不支持 ECDHE，我们推荐使用 RSA 而不是 DH 或者 DHE，因为 DH 系列算法非常消耗 CPU（相当于要做两次 RSA 计算）。 需要注意通常所说的 ECDHE 密钥交换默认都是指 ECDHE_RSA，使用 ECDHE 生成 DH 算法所需的公私钥，然后使用 RSA 算法进行签名最后再计算得出对称密钥。非对称加密相比对称加密更加安全，但也存在两个明显缺点： CPU 计算资源消耗非常大。一次完全 TLS 握手，密钥交换时的非对称解密计算量占整个握手过程的 90% 以上。而对称加密的计算量只相当于非对称加密的 0.1%，如果应用层数据也使用非对称加解密，性能开销太大，无法承受。 非对称加密算法对加密内容的长度有限制，不能超过公钥长度。比如现在常用的公钥长度是 2048 位，意味着待加密内容不能超过 256 个字节。所以公钥加密目前只能用来作密钥交换或者内容签名，不适合用来做应用层传输内容的加解密。 非对称密钥交换算法是整个 HTTPS 得以安全的基石，充分理解非对称密钥交换算法是理解 HTTPS 协议和功能的关键。下面分别通俗地介绍一下 RSA 和 ECDHE 在密钥交换过程中的应用。 RSA 在密钥交换过程中的应用 RSA 算法的原理是乘法不可逆或者大数因子很难分解。RSA 的推导实现涉及到了欧拉函数和费马定理及模反元素的概念，有兴趣的读者可以自行百度。 RSA 算法是统治世界的最重要算法之一，而且从目前来看，RSA 也是 HTTPS 体系中最重要的算法，没有之一。 下面用一个简单的示例介绍一下 RSA 的神奇妙用。 假设一个网站需要使用 HTTPS 协议，那么它首先就得申请数字证书，申请证书之前需要生成一对公钥和私钥，为了方便说明问题，假设 server 的密钥长度只有 8 位，事实上现在的服务器证书至少是 2048 位长。 随机挑选两个质数 p, q，使得 pq 接近 2 的 8 次方 = 256， 假设 p = 13, q = 19。n = pq = 13*19 = 247。 挑选一个数 e，满足 1(q-1) 并且 e 与 (p-1)(q-1) 互质，假设 e = 53。 计算 e 关于 n 的模反元素 , ed≡1 (mod φ(n))， d = 实际应用中，(n,e) 组成了公钥对，（n,d）组成了私钥对。公钥一般都注册到了证书里，任何人都能直接查看，比如百度证书的公钥对如下图，其中最末 6 个数字（010001）换算成 10 进制就是 65537，也就是公钥对中的 e, 取值比较小的原因有两个： 减小 client 端的计算强度，特别是现在移动终端的计算能力比较弱，较小的公钥使得 CPU 计算会更快。 加大 server 端的破解难度。e 比较小，d 必然会非常大。所以 d 的取值空间也会非常大。 ECDHE 算法在密钥交换中的应用 ECDHE 算法实现要复杂很多，依赖的数学原理主要是 ECC 椭圆曲线和离散对数。详细概念不做说明，示例介绍一下。 对称内容加密 非对称密钥交换过程结束之后就得出了本次会话需要使用的对称密钥。对称加密又分为两种模式：流式加密和分组加密。流式加密现在常用的就是 RC4，不过 RC4 已经不再安全，微软也建议网站尽量不要使用 RC4 流式加密。一种新的替代 RC4 的流式加密算法叫 ChaCha20，它是 google 推出的速度更快，更安全的加密算法。目前已经被 android 和 chrome 采用，也编译进了 google 的开源 openssl 分支---boring ssl，并且 nginx 1.7.4 也支持编译 boringssl。分组加密以前常用的模式是 AES-CBC，但是 CBC 已经被证明容易遭受 BEAST 和 LUCKY13 攻击。目前建议使用的分组加密模式是 AES-GCM，不过它的缺点是计算量大，性能和电量消耗都比较高，不适用于移动电话和平板电脑。 数据完整性 这部分内容比较好理解，跟平时的 md5 签名类似，只不过安全要求要高很多。openssl 现在使用的完整性校验算法有两种：MD5 或者 SHA。由于 MD5 在实际应用中存在冲突的可能性比较大，所以尽量别采用 MD5 来验证内容一致性。SHA 也不能使用 SHA0 和 SHA1，中国山东大学的王小云教授在 2005 年就宣布破解了 SHA-1 完整版算法。微软和 google 都已经宣布 16 年及 17 年之后不再支持 sha1 签名证书。 身份认证 身份认证主要涉及到 PKI 和数字证书。数字证书有两个作用： 身份授权。确保浏览器访问的网站是经过 CA 验证的可信任的网站。 分发公钥。每个数字证书都包含了注册者生成的公钥。在 SSL 握手时会通过 certificate 消息传输给客户端。 这里简单介绍一下数字证书是如何验证网站身份的，PKI 体系的具体知识不做详细介绍。证书申请者首先会生成一对密钥，包含公钥和密钥，然后把公钥及域名还有 CU 等资料制作成 CSR 格式的请求发送给 RA，RA 验证完这些内容之后（RA 会请独立的第三方机构和律师团队确认申请者的身份）再将 CSR 发送给 CA，CA 然后制作 X.509 格式的证书。 申请者拿到 CA 的证书并部署在网站服务器端，那浏览器发起握手接收到证书后，如何确认这个证书就是 CA 签发的呢？怎样避免第三方伪造这个证书？答案就是数字签名（digital signature）。数字签名可以认为是一个证书的防伪标签，目前使用最广泛的 SHA-RSA 数字签名的制作和验证过程如下： 数字签名的签发。首先是使用哈希函数对证书数据哈希，生成消息摘要，然后使用 CA 自己的私钥对证书内容和消息摘要进行加密。 数字签名的校验。使用 CA 的公钥解密签名，然后使用相同的签名函数对证书内容进行签名并和服务端的数字签名里的签名内容进行比较，如果相同就认为校验成功。 这里有几点需要说明： 数字签名签发和校验使用的密钥对是 CA 自己的公私密钥，跟证书申请者提交的公钥没有关系。 数字签名的签发过程跟公钥加密的过程刚好相反，即是用私钥加密，公钥解密。 现在大的 CA 都会有证书链，证书链的好处一是安全，保持根 CA 的私钥离线使用。第二个好处是方便部署和撤销，即如何证书出现问题，只需要撤销相应级别的证书，根证书依然安全。 根 CA 证书都是自签名，即用自己的公钥和私钥完成了签名的制作和验证。而证书链上的证书签名都是使用上一级证书的密钥对完成签名和验证的。 怎样获取根 CA 和多级 CA 的密钥对？它们是否可信？当然可信，因为这些厂商跟浏览器和操作系统都有合作，它们的公钥都默认装到了浏览器或者操作系统环境里。比如 firefox 就自己维护了一个可信任的 CA 列表，而 chrome 和 IE 使用的是操作系统的 CA 列表。 HTTPS 使用成本 HTTPS 目前唯一的问题就是它还没有得到大规模应用，受到的关注和研究都比较少。至于使用成本和额外开销，完全不用太过担心。一般来讲，使用 HTTPS 前大家可能会非常关注如下问题： 证书费用以及更新维护。大家觉得申请证书很麻烦，证书也很贵，可是证书其实一点都不贵，便宜的一年几十块钱，最多也就几百。而且现在也有了免费的证书机构，比如著名的 mozilla 发起的免费证书项目：let’s encrypt就支持免费证书安装和自动更新。这个项目将于今年中旬投入正式使用。 数字证书的费用其实也不高，对于中小网站可以使用便宜甚至免费的数字证书服务（可能存在安全隐患），像著名的 verisign 公司的证书一般也就几千到几万块一年不等。当然如果公司对证书的需求比较大，定制性要求高，可以建立自己的 CA 站点，比如 google，能够随意签发 google 相关证书。 HTTPS 降低用户访问速度。HTTPS 对速度会有一定程度的降低，但是只要经过合理优化和部署，HTTPS 对速度的影响完全可以接受。在很多场景下，HTTPS 速度完全不逊于 HTTP，如果使用 SPDY，HTTPS 的速度甚至还要比 HTTP 快。 大家现在使用百度 HTTPS 安全搜索，有感觉到慢吗？ HTTPS 消耗 CPU 资源，需要增加大量机器。前面介绍过非对称密钥交换，这是消耗 CPU 计算资源的大户，此外，对称加解密，也需要 CPU 的计算。 同样地，只要合理优化，HTTPS 的机器成本也不会明显增加。对于中小网站，完全不需要增加机器也能满足性能需求。 后记 国外的大型互联网公司很多已经启用了全站 HTTPS，这也是未来互联网的趋势。国内的大型互联网并没有全站部署 HTTPS，只是在一些涉及账户或者交易的子页面 / 子请求上启用了 HTTPS。百度搜索首次全站部署 HTTPS，对国内互联网的全站 HTTPS 进程必将有着巨大的推动作用。目前互联网上关于 HTTPS 的中文资料比较少，本文就着重介绍了 HTTPS 协议涉及到的重要知识点和平时不太容易理解的盲区，希望能对大家理解 HTTPS 协议有帮助。百度 HTTPS 性能优化涉及到大量内容，从前端页面、后端架构、协议特性、加密算法、流量调度、架构和运维、安全等方面都做了大量工作。本系列的文章将一一进行介绍。 "},"security/https-pratice-2.html":{"url":"security/https-pratice-2.html","title":"大型网站的HTTPS实践（二）-- HTTPS对性能的影响","keywords":"","body":"大型网站的 HTTPS 实践（二）-- HTTPS 对性能的影响 前言 HTTPS 在保护用户隐私，防止流量劫持方面发挥着非常关键的作用，但与此同时，HTTPS 也会降低用户访问速度，增加网站服务器的计算资源消耗。本文主要介绍 https 对用户体验的影响。 HTTPS 对访问速度的影响 在介绍速度优化策略之前，先来看下 HTTPS 对速度有什么影响。影响主要来自两方面： 协议交互所增加的网络 RTT(round trip time)。 加解密相关的计算耗时。 下面分别介绍一下。 网络耗时增加 由于 HTTP 和 HTTPS 都需要 DNS 解析，并且大部分情况下使用了 DNS 缓存，为了突出对比效果，忽略主域名的 DNS 解析时间。用户使用 HTTP 协议访问 http://www.baidu.com(或者 www.baidu.com) 时会有如下网络上的交互耗时： 图 1 HTTP 首个请求的网络耗时可见，用户只需要完成 TCP 三次握手建立 TCP 连接就能够直接发送 HTTP 请求获取应用层数据，此外在整个访问过程中也没有需要消耗计算资源的地方。接下来看 HTTPS 的访问过程，相比 HTTP 要复杂很多，在部分场景下，使用 HTTPS 访问有可能增加 7 个 RTT。如下图： 图 2 HTTPS 首次请求对访问速度的影响HTTPS 首次请求需要的网络耗时解释如下： 三次握手建立 TCP 连接。耗时一个 RTT。 使用 HTTP 发起 GET 请求，服务端返回 302 跳转到 https://www.baidu.com 。需要一个 RTT 以及 302 跳转延时。 大部分情况下用户不会手动输入 https://www.baidu.com 来访问 HTTPS，服务端只能返回 302 强制浏览器跳转到 https。 浏览器处理 302 跳转也需要耗时。 三次握手重新建立 TCP 连接。耗时一个 RTT。 302 跳转到 HTTPS 服务器之后，由于端口和服务器不同，需要重新完成三次握手，建立 TCP 连接。 TLS 完全握手阶段一。耗时至少一个 RTT。 这个阶段主要是完成加密套件的协商和证书的身份认证。 服务端和浏览器会协商出相同的密钥交换算法、对称加密算法、内容一致性校验算法、证书签名算法、椭圆曲线（非 ECC 算法不需要）等。 浏览器获取到证书后需要校验证书的有效性，比如是否过期，是否撤销。 解析 CA 站点的 DNS。耗时一个 RTT。 浏览器获取到证书后，有可能需要发起 OCSP 或者 CRL 请求，查询证书状态。 浏览器首先获取证书里的 CA 域名。 如果没有命中缓存，浏览器需要解析 CA 域名的 DNS。 三次握手建立 CA 站点的 TCP 连接。耗时一个 RTT。 DNS 解析到 IP 后，需要完成三次握手建立 TCP 连接。 发起 OCSP 请求，获取响应。耗时一个 RTT。 完全握手阶段二，耗时一个 RTT 及计算时间。 完全握手阶段二主要是密钥协商。 完全握手结束后，浏览器和服务器之间进行应用层（也就是 HTTP）数据传输。 当然不是每个请求都需要增加 7 个 RTT 才能完成 HTTPS 首次请求交互。大概只有不到 0.01% 的请求才有可能需要经历上述步骤，它们需要满足如下条件： 必须是首次请求。即建立 TCP 连接后发起的第一个请求，该连接上的后续请求都不需要再发生上述行为。 必须要发生完全握手，而正常情况下 80% 的请求能实现简化握手。 浏览器需要开启 OCSP 或者 CRL 功能。Chrome 默认关闭了 ocsp 功能，firefox 和 IE 都默认开启。 浏览器没有命中 OCSP 缓存。Ocsp 一般的更新周期是 7 天，firefox 的查询周期也是 7 天，也就说是 7 天中才会发生一次 ocsp 的查询。 浏览器没有命中 CA 站点的 DNS 缓存。只有没命中 DNS 缓存的情况下才会解析 CA 的 DNS。 2.2 计算耗时增加 上节还只是简单描述了 HTTPS 关键路径上必须消耗的纯网络耗时，没有包括非常消耗 CPU 资源的计算耗时，事实上计算耗时也不小（30ms 以上），从浏览器和服务器的角度分别介绍一下： 浏览器计算耗时 RSA 证书签名校验，浏览器需要解密签名，计算证书哈希值。如果有多个证书链，浏览器需要校验多个证书。 RSA 密钥交换时，需要使用证书公钥加密 premaster。耗时比较小，但如果手机性能比较差，可能也需要 1ms 的时间。 ECC 密钥交换时，需要计算椭圆曲线的公私钥。 ECC 密钥交换时，需要使用证书公钥解密获取服务端发过来的 ECC 公钥。 ECC 密钥交换时，需要根据服务端公钥计算 master key。 应用层数据对称加解密。 应用层数据一致性校验。 服务端计算耗时 RSA 密钥交换时需要使用证书私钥解密 premaster。这个过程非常消耗性能。 ECC 密钥交换时，需要计算椭圆曲线的公私钥。 ECC 密钥交换时，需要使用证书私钥加密 ECC 的公钥。 ECC 密钥交换时，需要根据浏览器公钥计算共享的 master key。 应用层数据对称加解密。 应用层数据一致性校验。 由于客户端的 CPU 和操作系统种类比较多，所以计算耗时不能一概而论。手机端的 HTTPS 计算会比较消耗性能，单纯计算增加的延迟至少在 50ms 以上。PC 端也会增加至少 10ms 以上的计算延迟。服务器的性能一般比较强，但由于 RSA 证书私钥长度远大于客户端，所以服务端的计算延迟也会在 5ms 以上。 结束语 本系列的后续文章将进一步解释针对性的优化措施。 "},"security/https-pratice-3.html":{"url":"security/https-pratice-3.html","title":"大型网站的HTTPS实践（三）-- 基于协议和配置的优化","keywords":"","body":"大型网站的 HTTPS 实践（三）-- 基于协议和配置的优化 前言 上文讲到 HTTPS 对用户访问速度的影响。本文就为大家介绍 HTTPS 在访问速度，计算性能，安全等方面基于协议和配置的优化。 HTTPS 访问速度优化 Tcp fast open HTTPS 和 HTTP 使用 TCP 协议进行传输，也就意味着必须通过三次握手建立 TCP 连接，但一个 RTT 的时间内只传输一个 syn 包是不是太浪费？能不能在 syn 包发出的同时捎上应用层的数据？其实是可以的，这也是 tcp fast open 的思路，简称 TFO。具体原理可以参考 rfc7413。遗憾的是 TFO 需要高版本内核的支持，linux 从 3.7 以后支持 TFO，但是目前的 windows 系统还不支持 TFO，所以只能在公司内部服务器之间发挥作用。 HSTS 前面提到过将用户 HTTP 请求 302 跳转到 HTTPS，这会有两个影响： 不安全，302 跳转不仅暴露了用户的访问站点，也很容易被中间者支持。 降低访问速度，302 跳转不仅需要一个 RTT，浏览器执行跳转也需要执行时间。由于 302 跳转事实上是由浏览器触发的，服务器无法完全控制，这个需求导致了 HSTS 的诞生：HSTS(HTTP Strict Transport Security)。服务端返回一个 HSTS 的 http header，浏览器获取到 HSTS 头部之后，在一段时间内，不管用户输入www.baidu.com还是http://www.baidu.com，都会默认将请求内部跳转成https://www.baidu.com。Chrome, firefox, ie 都支持了 HSTS（http://caniuse.com/#feat=stricttransportsecurity）。 Session resume Session resume 顾名思义就是复用 session，实现简化握手。复用 session 的好处有两个： 减少了 CPU 消耗，因为不需要进行非对称密钥交换的计算。 提升访问速度，不需要进行完全握手阶段二，节省了一个 RTT 和计算耗时。 TLS 协议目前提供两种机制实现 session resume，分别介绍一下。 Session cache Session cache 的原理是使用 client hello 中的 session id 查询服务端的 session cache, 如果服务端有对应的缓存，则直接使用已有的 session 信息提前完成握手，称为简化握手。Session cache 有两个缺点： 需要消耗服务端内存来存储 session 内容。 目前的开源软件包括 nginx,apache 只支持单机多进程间共享缓存，不支持多机间分布式缓存，对于百度或者其他大型互联网公司而言，单机 session cache 几乎没有作用。 Session cache 也有一个非常大的优点： session id 是 TLS 协议的标准字段，市面上的浏览器全部都支持 session cache。 百度通过对 TLS 握手协议及服务器端实现的优化，已经支持全局的 session cache，能够明显提升用户的访问速度，节省服务器计算资源。 Session ticket 上节提到了 session cache 的两个缺点，session ticket 能够弥补这些不足。Session ticket 的原理参考 RFC4507。简述如下：server 将 session 信息加密成 ticket 发送给浏览器，浏览器后续握手请求时会发送 ticket，server 端如果能成功解密和处理 ticket，就能完成简化握手。显然，session ticket 的优点是不需要服务端消耗大量资源来存储 session 内容。Session ticket 的缺点： session ticket 只是 TLS 协议的一个扩展特性，目前的支持率不是很广泛，只有 60% 左右。 session ticket 需要维护一个全局的 key 来加解密，需要考虑 KEY 的安全性和部署效率。 总体来讲，session ticket 的功能特性明显优于 session cache。希望客户端实现优先支持 session ticket。 Ocsp stapling Ocsp 全称在线证书状态检查协议 (rfc6960)，用来向 CA 站点查询证书状态，比如是否撤销。通常情况下，浏览器使用 OCSP 协议发起查询请求，CA 返回证书状态内容，然后浏览器接受证书是否可信的状态。这个过程非常消耗时间，因为 CA 站点有可能在国外，网络不稳定，RTT 也比较大。那有没有办法不直接向 CA 站点请求 OCSP 内容呢？ocsp stapling 就能实现这个功能。详细介绍参考 RFC6066 第 8 节。简述原理就是浏览器发起 client hello 时会携带一个 certificate status request 的扩展，服务端看到这个扩展后将 OCSP 内容直接返回给浏览器，完成证书状态检查。由于浏览器不需要直接向 CA 站点查询证书状态，这个功能对访问速度的提升非常明显。Nginx 目前已经支持这个 ocsp stapling file，只需要配置 ocsp stapling file 的指令就能开启这个功能： False start 通常情况下，应用层数据必须等完全握手全部结束之后才能传输。这个其实比较浪费时间，那能不能类似 TFO 一样，在完全握手的第二个阶段将应用数据一起发出来呢？google 提出了 false start 来实现这个功能。详细介绍参考 https://tools.ietf.org/html/draft-bmoeller-tls-falsestart-00。简单概括 False start 的原理就是在 client_key_exchange 发出时将应用层数据一起发出来，能够节省一个 RTT。False start 依赖于 PFS（perfect forward secrecy 完美前向加密），而 PFS 又依赖于 DHE 密钥交换系列算法（DHE_RSA, ECDHE_RSA, DHE_DSS, ECDHE_ECDSA），所以尽量优先支持 ECDHE 密钥交换算法实现 false start。 使用 SPDY 或者 HTTP2 SPDY 是 google 推出的优化 HTTP 传输效率的协议（https://www.chromium.org/spdy），它基本上沿用了 HTTP 协议的语义 , 但是通过使用帧控制实现了多个特性，显著提升了 HTTP 协议的传输效率。SPDY 最大的特性就是多路复用，能将多个 HTTP 请求在同一个连接上一起发出去，不像目前的 HTTP 协议一样，只能串行地逐个发送请求。Pipeline 虽然支持多个请求一起发送，但是接收时依然得按照顺序接收，本质上无法解决并发的问题。HTTP2 是 IETF 2015 年 2 月份通过的 HTTP 下一代协议，它以 SPDY 为原型，经过两年多的讨论和完善最终确定。本文就不过多介绍 SPDY 和 HTTP2 的收益，需要说明两点： SPDY 和 HTTP2 目前的实现默认使用 HTTPS 协议。 SPDY 和 HTTP2 都支持现有的 HTTP 语义和 API，对 WEB 应用几乎是透明的。Google 宣布 chrome 浏览器 2016 年将放弃 SPDY 协议，全面支持 HTTP2，但是目前国内部分浏览器厂商进度非常慢，不仅不支持 HTTP2，连 SPDY 都没有支持过。百度服务端和百度手机浏览器现在都已经支持 SPDY3.1 协议。 HTTPS 计算性能优化 优先使用 ECC ECC 椭圆加密算术相比普通的离散对数计算速度性能要强很多。下表是 NIST 推荐的密钥长度对照表。对称密钥大小 | RSA 和 DH 密钥大小 | ECC 密钥大小----|------|---- 80|1024|160| 112|2048|224 128|3072|256 192|7680|384 256|15360|521 表格 2 NIST 推荐使用的密钥长度对于 RSA 算法来讲，目前至少使用 2048 位以上的密钥长度才能保证安全性。ECC 只需要使用 224 位长度的密钥就能实现 RSA2048 位长度的安全强度。在进行相同的模指数运算时速度显然要快很多。 使用最新版的 openssl 一般来讲，新版的 openssl 相比老版的计算速度和安全性都会有提升。比如 openssl1.0.2 采用了 intel 最新的优化成果，椭圆曲线 p256 的计算性能提升了 4 倍。(https://eprint.iacr.org/2013/816.pdf)Openssl 2014 年就升级了 5 次，基本都是为了修复实现上的 BUG 或者算法上的漏洞而升级的。所以尽量使用最新版本，避免安全上的风险。 硬件加速方案 现在比较常用的 TLS 硬件加速方案主要有两种： SSL 专用加速卡。 GPU SSL 加速。 上述两个方案的主流用法都是将硬件插入到服务器的 PCI 插槽中，由硬件完成最消耗性能的计算。但这样的方案有如下缺点： 支持算法有限。比如不支持 ECC，不支持 GCM 等。 升级成本高。 出现新的加密算法或者协议时，硬件加速方案无法及时升级。 出现比较大的安全漏洞时，部分硬件方案在无法在短期内升级解决。比如 2014 年暴露的 heartbleed 漏洞。 无法充分利用硬件加速性能。硬件加速程序一般都运行在内核态，计算结果传递到应用层需要 IO 和内存拷贝开销，即使硬件计算性能非常好，上层的同步等待和 IO 开销也会导致整体性能达不到预期，无法充分利用硬件加速卡的计算能力。 维护性差。硬件驱动及应用层 API 大部分是由安全厂家提供，出现问题后还需要厂家跟进。用户无法掌握核心代码，比较被动。不像开源的 openssl，不管算法还是协议，用户都能掌握。 TLS 远程代理计算 也正是因为上述原因，百度实现了专用的 SSL 硬件加速集群。基本思路是： 优化 TLS 协议栈，剥离最消耗 CPU 资源的计算，主要有如下部分： RSA 中的加解密计算。 ECC 算法中的公私钥生成。 ECC 算法中的共享密钥生成。 优化硬件计算部分。硬件计算不涉及协议及状态交互，只需要处理大数运算。 Web server 到 TLS 计算集群之间的任务是异步的。即 web server 将待计算内容发送给加速集群后，依然可以继续处理其他请求，整个过程是异步非阻塞的。 HTTPS 安全配置 协议版本选择 SSL2.0 早就被证明是不安全的协议了，统计发现目前已经没有客户端支持 SSL2.0，所以可以放心地在服务端禁用 SSL2.0 协议。2014 年爆发了 POODLE 攻击，SSL3.0 因此被证明是不安全的。但是统计发现依然有 0.5% 的流量只支持 SSL3.0。所以只能有选择地支持 SSL3.0。TLS1.1 及 1.2 目前为止没有发现安全漏洞，建议优先支持。 加密套件选择 加密套件包含四个部分： 非对称密钥交换算法。建议优先使用 ECDHE，禁用 DHE，次优先选择 RSA。 证书签名算法。由于部分浏览器及操作系统不支持 ECDSA 签名，目前默认都是使用 RSA 签名，其中 SHA1 签名已经不再安全，chrome 及微软 2016 年开始不再支持 SHA1 签名的证书 (http://googleonlinesecurity.blogspot.jp/2014/09/gradually-sunsetting-sha-1.html)。 对称加解密算法。优先使用 AES-GCM 算法，针对 1.0 以上协议禁用 RC4（ rfc7465）。 内容一致性校验算法。Md5 和 sha1 都已经不安全，建议使用 sha2 以上的安全哈希函数。 HTTPS 防攻击 防止协议降级攻击 降级攻击一般包括两种：加密套件降级攻击 (cipher suite rollback) 和协议降级攻击（version roll back）。降级攻击的原理就是攻击者伪造或者修改 client hello 消息，使得客户端和服务器之间使用比较弱的加密套件或者协议完成通信。为了应对降级攻击，现在 server 端和浏览器之间都实现了 SCSV 功能，原理参考 https://tools.ietf.org/html/draft-ietf-tls-downgrade-scsv-00。一句话解释就是如果客户端想要降级，必须发送 TLS_SCSV 的信号，服务器如果看到 TLS_SCSV，就不会接受比服务端最高协议版本低的协议。 防止重新协商攻击 重新协商（tls renegotiation）分为两种：加密套件重协商 (cipher suite renegotiation) 和协议重协商（protocol renegotiation）。重新协商会有两个隐患： 重协商后使用弱的安全算法。这样的后果就是传输内容很容易泄露。 重协商过程中不断发起完全握手请求，触发服务端进行高强度计算并引发服务拒绝。 对于重协商，最直接的保护手段就是禁止客户端主动重协商，当然出于特殊场景的需求，应该允许服务端主动发起重协商。 结束语 HTTPS 的实践和优化涉及到了非常多的知识点，由于篇幅关系，本文对很多优化策略只是简单介绍了一下 . 如果想要了解协议背后的原理，还是需要详细阅读 TLS 协议及 PKI 知识。对于大型站点来说，如果希望做到极致，HTTPS 的部署需要结合产品和基础设施的架构来进行详细的考虑，比起部署支持 HTTPS 的接入和对它的优化，在产品和运维层面上花费的功夫会更多。本系列的下一篇文章将进一步进行介绍。 "},"security/https-pratice-4.html":{"url":"security/https-pratice-4.html","title":"大型网站的HTTPS实践（四）-- 协议层以外的实践[1]","keywords":"","body":"大型网站的 HTTPS 实践（四）-- 协议层以外的实践 前言 网上介绍 https 的文章并不多，更鲜有分享在大型互联网站点部署 https 的实践经验，我们在考虑部署 https 时也有重重的疑惑。本文为大家介绍百度 HTTPS 的实践和一些权衡 , 希望以此抛砖引玉。 协议层以外的实践工作 全站覆盖 https 的理由 很多刚接触 https 的会思考，我是不是只要站点的主域名换了 https 就可以？答案是不行。https 的目的就是保证传输过程的安全，如果只有主域名上了 https，但是主域名加载的资源，比如 js，css，图片没有上 https，会怎么样？从效果上来说，没有达到保证网站传输过程安全的目的，因为你的 js，css，图片仍然有被劫持的可能性，如果这些内容被篡改 / 嗅探了，那么 https 的意义就失去了。浏览器在设计上早就考虑的这样的情况，会有相应的提示。具体的实现依赖浏览器，例如地址栏锁形标记从绿色变为黄色 , 阻止这次请求，或者直接弹出非常影响用户体验的提示 (主要是 IE)，用户会感觉厌烦，疑惑和担忧安全性。 很多用户看见这个链接会习惯性的点” 是”，这样非 https 的资源就被禁止加载了。非 ie 的浏览器很多也会阻止加载一些危害程度较高的非 https 资源（例如 js）。我们发现移动端浏览器的限制目前会略松一些。所以这里要是没做好，很多情况连网站的基本功能都没法正常使用。 站点的区别 很多人刚接触 https 的时候，觉得不就是部署证书，让 webserver 支持 https 就行了吗。实际上对于不同的站点来说，https 的部署方式和难度有很大的区别。对于一个大型站点来说，让 webserver 支持 https，以及对 webserver 在 https 协议特性上做一些优化，在迁移的工作比重上，可能只占到 20%-40%。我们考虑下以下几种情况下，部署 https 的方案。 简单的个人站点 简单的定义：资源只从本站的主域或者主域的子域名加载。比如 axyz 的个人 blog，域名是 axyzblog.com。加载主域名下的 js 和图片。 这样的站部署 https，在已有证书且 webserver 支持的情况下，只需要把主域名替换为 https 接入，然后把资源连接修改为 https:// 或者 //。 复杂的个人站点 复杂的定义：资源需要从外部域名加载。 这样就比较麻烦了，主域资源容易适配 https，在 cdn 上加载的资源还需要 cdn 服务商支持 https。目前各大 cdn 的服务商正在逐渐提供 https 的支持，需要迁移的朋友可以看看自己使用的 cdn 是否提供了这项能力。一些 cdn 会对 https 流量额外收费。 Cdn 使用 https 常见的方案有： 网站主提供私钥给 cdn，回源使用 http。 cdn 使用公共域名，公共的证书，这样资源的域名就不能自定义了。回源使用 http。 仅提供动态加速，cdn 进行 tcp 代理，不缓存内容。 CloudFlare 提供了 Keyless SSL 的服务，能够支持不愿意提供私钥 , 不想使用公共的域名和证书却又需要使用 cdn 的站点了。 简单的大型站点 简单的定义: 资源只从本站的主域 , 主域的子域，或者自建 / 可控的 cdn 域名加载，几乎没有第三方资源。如果网站本身的特性就如此，或愿意改造为这样的类型，部署 https 就相对容易。Google Twitter 都是非常好的范例。优点：已经改成这样的站点，替换 https 就比较容易。缺点：如果需要改造，那么要很大的决心，毕竟几乎不能使用多样化的第三方资源了。 复杂，访问速度重要性稍低的大型站点 复杂的定义：从本站的非主域，或者第三方站点的域名有大量的第三方资源需要加载，多出现在一些平台类，或者有复杂内容展现的的网站。访问速度要求：用户停留时间长或者强需求，用户对访问速度的耐受程度较高。比如门户，视频，在线交易类（比如火车票 机票 商城）网站。这样的站点，可以努力推动所有相关域名升级为支持 https。我们用下图举例说明下这样修改会导致一个网站的链接发生怎样的改变。 负责流量接入的团队将可控的接入环境改造为 http 和 https 都支持，这样前端工程的工作相对就少一些。大部分时候将链接从 http:// 替换为 // 即可 . 在主域名是 https 的情况下，其它资源就能自动从 https 协议下加载。一些第三方资源怎么办？一般来说只有两种选择，一迁移到自己的 cdn 或者 idc 吧，二强制要求第三方自己能支持 https。以全站 https 接入的 facebook 举例。第三方厂商想在 facebook 上线一个游戏。facebook：请提供 https 接入吧。第三方想：能赚钱啊，还是提供下 https 接入吧。所以，足够强势，有吸引力，合作方也有提供 https 的能力的话，这是完全可行的。如果你的平台接入的都是一些个人开发者，而且还赚不到多少钱的情况下，这样就行不通了。 优点：前端改动相对简单，不容易出现 https 下还有 http 的资源问题。 缺点：通常这样的实现下，用户的访问速度会变慢，比如从 2.5 秒变为 3 秒，如上述的理由，用户还是能接受的。对第三方要求高。 复杂，访问速度有严格要求的大型站点 复杂的定义：同上。 访问速度要求：停留时间不长，用户对访问速度的心理预期较高。 但是如果用户把网站当作工具使用，需要你很快给出响应的时候，这样的实现就不好了。后续几个部分我们介绍下这些优化的抉择。 域名的选择 域名对访问速度的影响具有两面性：域名多，域名解析和建立连接的时间就多；域名少，下载并发度又不够。https 下重建连接的时间成本比 http 更高，对于上面提到的简单的大型站点 , 可以只用 1-3 个域名就能满足需求，对于百度这样富展现样式较多的搜索引擎来说，页面可能展示的资源种类太多。而不同类型的资源又是由不同的域名 (不同的产品 或者第三方产品) 提供的服务，换一个词搜索就可能需要重新建立一些资源的 ssl 链接，会让用户感受到卡顿。 如果将域名限制在有限的范围 (一般 2-6 个左右)，维持和这些域名的连接，合并一些数据，加上有 spdy，http2.0 来保证并发，是可以满足我们的需求的。我们的现状是：百度搜索有数百个资源域名在加载各类的资源。这就变成了如何解决这样的问题：如何用 2-6 个有限的域名提供数百个域名的服务，这就涉及了下一节，代理接入与 cdn。 代理接入 当域名从数百域名缩减到个位数的时候，就不可避免的需要谈到统一接入，流量转发和调度等内容。通常的站点资源大都是从主域名 +cdn 进行加载，所以我们可以把域名分为这两类，进行替换。 替换后的几个 cdn 域名都指向相同的 cname，这样的话意味着用户访问的途径变为如下的方式。 这样 ssl 的握手只在用户和两类节点之间进行，维持连接相对容易很多，也不需要每个域名都去申请证书，部署 https 接入。这个方式会遇到域名转换，数据透传，流量调度等一系列的问题，需要进行整体设计架构，对很多细节都需要进行优化，在运维和研发上都有不小的投入。理想的方式：这样就只需要和 cdn 节点进行 https 握手，大幅缩短了握手的 rtt 时间 (cdn 节点一般广泛的分布在离用户很近的地方，而主域节点一般都比较有限). 这样部署会对 cdn 的运维和研发能力有更高的要求。 大家有没发现，这样的接入就把一个复杂的站点变为简单的站点了？ 连接复用 连接复用率可以分为 tcp 和 ssl 等不同的层面，需要分开进行分析和统计。 连接复用的意义 HTTP 协议 (RFC2616) 规定一个域名最多不能建立超过 2 个的 TCP 连接。但是随着互联网的发展，一张网页的元素越来越多，传输内容越来越大，一个域名 2 个连接的限制已经远远不能满足现在网页加载速度的需求。目前已经没有浏览器遵守这个规定，各浏览器针对单域名建立的 TCP 连接数如下： 浏览器 连接数 Firefox 2 2 Firefox 3+ 6 Chrome 6 Ie10 8 IE8 6 Safari 5 6 Opera 12 6 表格 1 浏览器单域名建立的最大并发连接数 从上表看出，单个域名的连接数基本上是 6 个。所以只能通过增加域名的方式来增加并发连接数。在 HTTP 场景下，这样的方式没有什么问题。但是在 HTTPS 连接下，由于 TLS 连接建立的成本比较高，增加并发连接数本身就会带来较大的延迟，所以对域名数需要一个谨慎的控制。特别是 HTTP2 即将大规模应用，而 HTTP2 的最大特性就是多路复用，使用多个域名和多个连接无法有效发挥多路复用和压缩的特性。 那 HTTPS 协议下，一张网页到底该有多少域名呢？这个其实没有定论，取决于网页需要加载元素个数。 预建连接 既然从协议角度无法减少握手对速度的影响，那能不能提前建立连接，减少用户可以感知的握手延迟呢？当然是可以的。思路就是预判当前用户的下一个访问 URL，提前建立连接，当用户发起真实请求时，TCP 及 TLS 握手都已经完成，只需要在连接上发送应用层数据即可。 最简单有效的方式就是在主域下对连接进行预建，可以通过请求一些静态资源的方式。但是这样还是不容易做到极致，因为使用哪个连接，并发多少还是浏览器控制的。例如你对 a 域名请求一个图片，浏览器建立了两个连接，再请求一张图片的时候，浏览器很大概率能够复用连接，但是当 a 域名需要加载 10 个图片的时候，浏览器很可能就会新建连接了。 Spdy 的影响 Spdy 对于连接复用率的提升非常有效，因为它能支持连接上的并发请求，所以浏览器会尽量在这个链接上保持复用。 其它 也可以尝试一些其他发方法，让浏览器在访问你的网站之前就建立过 https 连接，这样 session 能够复用。HSTS 也能有效的减少跳转时间，可惜对于复杂的网站来说，开启需要考虑清楚很多问题。 优化的效果 从百度的优化经验来看看，如果不开启 HSTS，用户在浏览器直接访问主域名，再通过 302 跳转到 HTTPS。增加的时间平均会有 400ms+，其中 302 跳转和 ssl 握手的因素各占一半。但是对于后续的请求，我们做到了对绝大部分用户几乎无感知。这 400ms+ 还有很多可以优化的空间，我们会持续优化用户的体验。 HTTPS 迁移遇到的一些常见问题。 传递 Referrer 我们可以把自己的网站替换为 https，但是一般的站点都有外链，要让外链都 https 目前还不太现实。很多网站需要从 referrer 中判断流量来源，因此对于搜索引擎这样的网站来说，referer 的传递还是比较重要的。如果不做任何设置，你会发现在 https 站点中点击外链并没有将 referrer 带入到 http 请求的头部中（http://tools.ietf.org/html/rfc7231#section-5.5.2）。现代的浏览器可以用 meta 标签来传递 refer。(http://w3c.github.io/webappsec/specs/referrer-policy) 传递完整的 url只传递站点，不包含路径和参数等。 对于不支持 meta 传递 referrer 的浏览器，例如 IE8, 我们怎么办呢？可以采用再次跳转的方法，既然 HTTPS 下不能给 HTTP 传递 referer，我们可以先从 HTTPS 访问一个可控的 http 站点，把需要传递的内容放到这个 http 站点的 url 中，然后再跳转到目标地址。 form 提交 有时需要将 form 提交到第三方站点，而第三方站点又是 http 的地址，浏览器会有不安全的警告。可以和 referrer 的跳转传递采取相似的逻辑。但是这样对 referer 和 form 等内容的方案，并不是完美的解决方法，因为这样还是增加了不安全的因素（劫持，隐私泄露等 ）。理想情况需要用户升级符合最新规范的浏览器，以及推进更多的站点迁移至 https。 视频播放 简单来说，如果你使用 http 的协议来播放视频，那么浏览器仍然会有不安全的提示。所以你有两种选择，1 让视频源提供 https。2 使用非 http 的协议，如 rtmp 协议。 用户异常 在 https 迁移的过程中，也会有不少热心的用户向我们反馈遇到的各种问题。 常见的有以下的一些情况： 用户的系统时间设置错误，导致提示证书过期。 用户使用 fiddler 等代理进行调试，但是没有添加这些软件的根证书，导致提示证书非法。 用户使用的 Dns 为公共 dns 或者跨网设置 dns，一些请求被运营商作为跨网流量拦截。 连通性有问题，我们发现一个小运营商的 https 失败率奇高，又没法联系到他们，只能不对他们进行 https 的转换。 慢。有时由于网络环境的因素，用户打开其他网站也慢，ping 哪个网站都要 500-2000ms。这时 https 自然也会很慢。 结束语 对于复杂的大型网站来说，HTTPS 的部署有很多工作要完成。 面对困难和挑战，有充足的动力支持着我们前进：https 上线后，劫持等原因导致的用户功能异常，隐私泄露的反馈大幅减少。热心的用户经常会向我们反馈遇到的各种问题。在以前，有时即使我们确定了是劫持的问题，能够解决问题的方法也非常有限。每当这种时候，自己总会产生一些无力感。HTTPS 的全站部署，给我们提供了能解决大部分问题的选项。能让一个做技术的人看到自己的努力解决了用户的问题，这就是最棒的收获。 HTTPS 没有想像中难用和可怕，只是没有经过优化。与大家共勉。 "},"security/https-faq.html":{"url":"security/https-faq.html","title":"HTTPS常见问题解答","keywords":"","body":"HTTPS 常见问题解答 1. 前言 百度从 14 年开始对外开放了 https 的访问，并于 3 月初正式对全网用户进行了 https 跳转。很多用户看到这个新闻都比较好奇，在新闻站点，微博，微信和贴吧，知乎等站点进行了热烈的讨论，这里我们也从一个普通用户容易理解的角度来看看大家的问题。 2 https 是什么？我有没有用到 https？ https 是 http over ssl(Secure Socket Layer)，简单讲就是 http 的安全版本，在 http 的基础上通过传输加密和身份认证保证了传输过程中的安全性。你通常访问的网站大部分都是 http 的，最简单的方法可以看看网址是以 http:// 开头还是 https:// 开头。以下几个截图就是 chrome,firefox,IE10 在使用 https 时的效果。 注意图中绿色的部分 , 我们后面详细说说。想进一步了解 HTTPS，可以阅读《大型网站的 HTTPS 实践（一）-- HTTPS 协议和原理》 3 https 为什么比 http 安全 ?https 加密 是不是需要我在电脑上安装证书 / 保存密码 ? http 不安全，主要是因为它传输的是明文内容 , 也不对传输双方进行身份验证。只要在数据传输路径的任何一个环节上，都能看到传输的内容，甚至对其进行修改。例如一篇文章”攻下隔壁女生路由器后 , 我都做了些什么” 中，很多攻击的环节，都是通过分析 http 的内容来进行。而在现实生活中呢，你很有可能泄露你的论坛高级会员账号 / 密码，游戏 vip 账号 / 密码，隐私的聊天内容，邮件，在线购物信息，等等。https 之所以安全，是因为他利用 ssl/tls 协议传输。举个简单的例子，电影风语者中，美军发现密码经常被日本窃听和破解，就征召了 29 名印第安纳瓦霍族人作为译电员，因为这语言只有他们族人懂。即使日本人窃听了电文，但是看不懂内容也没用；想伪造命令也无从下手，修改一些内容的话，印第安人看了，肯定会说看（shen）不（me）懂（gui）。看到这里，你肯定发现了，这是基于两边都有懂这个语言（加密解密规则）的人才行啊，那么我的电脑上需要安装什么密钥或者证书吗？一般情况作为普通用户是不用考虑这些的，我们有操作系统，浏览器，数学家，安全和网络工程师等等 , 帮你都做好了 , 放心的打开浏览器用就好啦。如果你实在好奇，想知道双方不用相同的密钥如何进行加密的，可以搜索下” 公钥加密”（非对称加密）,”RSA”,” DH 密钥交换”, “ssl 原理” “数字证书” 等关键词。有朋友会想了，不就是加密吗，我 wifi 密码都能破，找个工具分分钟就破解了。这个想法可不对 , 虽然没有绝对的安全，但是可以极大增加破解所需要的成本，https 目前使用的加密方式是需要巨大的计算量（按照目前计算机的计算能力）才可能破解的，你会用世界上最强的超级计算机花费 100 年（只是一个比喻）去解密，看看 100 年前隔壁老王在百度上搜什么吗。 4 百度为什么要上 https? 我们每天会处理用户投诉，比如说: 页面出现白页 / 出现某些奇怪的东西 返回了 403 的页面 搜索不了东西 搜索 url 带了小尾巴 , 页面总要闪几次 页面弹窗广告 搜索个汽车就有人给我打电话推销 4s 店和保险什么的 … 各种千奇百怪的情况 , 查来查去，很大一部分原因是有些坏人在数据的传输过程中修改百度的页面内容，窃听用户的搜索内容。悄悄告诉你，https 就是能解决这样问题的技术哦 , 赶紧把浏览器首页改成 https://www.baidu.com 吧。从方向上来说，HTTPS 也是未来的趋势，目前大家使用的 HTTP 还是 1.1/1.0 版本的，新的 HTTP2.0 版本的标准已经发布了。标准中涉及了加密的规范，虽然标准中没有强制使用，但是已经有很多浏览器实现声称他们只会支持基于加密连接的 HTTP2.0(https://http2.github.io/faq/#does-http2-require-encryption)。 5 https 不就是在 http 后面加个 s，很难么？ 难，又不难。它包含证书，卸载，流量转发，负载均衡，页面适配，浏览器适配，refer 传递等等等等。反正我指头肯定不够数。对于一个超小型个人站点来说，技术宅 1 天就能搞定从申请证书到改造完成。如果是从零开始建设，会更容易。但是对于百度搜索这种大胖纸来说，可就难了。 它一开始并不是为 https 设计的 内容丰富（内容本身的表现形式很多：图片，视频，flash，form 等等），种类丰富 (页面上除了自然结果，有视频，图片，地图，贴吧，百科 , 第三方的内容 , app 等等)。 数据来源复杂，有几十个内部产品线的内容，几百个域名，成千上万个开发者的内容 百度在全国，甚至世界范围都有很多 idc 和 cdn 节点，都得覆盖到。 还不能因此拖慢了百度的速度 (国内使用 https 的银行 , 在线交易的站点，有没有觉得很慢？) 上 https 本来就是为了更好的体验，可不能导致大家使用不稳定。…想了解更详细的内容，可以阅读《大型网站的 HTTPS 实践（四）-- 协议层以外的实践 [1]》Google 部署 https 花费了 1-2 年，13 年将证书从 1024 位升级到 2048 位花了 3 个月。百度也是去年就开放了入口和小流量，但是今年 3 月才进行全量上线，可以想像整体的复杂性。 6 如何看待百度搜索支持全站 https？ 国外的几个大型站点都 https 化了，这是未来互联网的趋势 (有兴趣的同学可以搜索下’http/2’ )。对百度自身来说，https 能够保护用户体验，减少劫持 / 隐私泄露对用户的伤害。很多人会有疑惑，我没有被劫持，百度上 https 有什么作用，反而让我变慢了一些。从我们的第一手数据可以看到，劫持的影响正越来越大，在法制不健全的环境下，它被当成一个产业，很多公司以它为生，不少以此创业的团队还拿到了风投。等它真正伤害到你的时候，你可能又会问我们为什么不做些什么。所以，我们宁愿早一些去面对它。https 在国内的大型站点目前还只用在部分账户的登陆和支付等环节。百度也是国内第一个全站 https 的大型站点，它的用户非常多，流量也很大。百度能够上线 https 会打消大家的疑虑，对其他国内的站点是很好的示范，这个带头作用会显著加速国内互联网 https 的进程，有助于中国互联网的网络安全建设。百度作为搜索引擎，是流量的入口和分发的渠道，后续如果对 https 的站点内容的抓取，标记，权值倾斜，那么更能引导互联网的网站向 https 进行迁移。 7 https 慢不慢 ? 如果什么优化都不做，https 会明显慢很多。在百度已经进行过很多速度优化的条件下，如果站点本身已经做过常规优化，但是不针对 https 做优化，这种情况下我们实测的结果是 0.2-0.4 秒耗时的增加。如果是没有优化过的站点，慢 1 秒都不是梦。至于现在慢不慢呢，大家已经体验了这么多天了，有感觉吗？答案：A 慢死了，你们在做啥 ? B 有些慢啊 C 还行 , 基本无感 D 啥 , 我已经用了 https 了？是不是选的 C 或者 D？喂喂，选 A 的那位 你打开别的网站慢么 , 以前没有上 HTTPS 的时候慢么。。。隔壁老王在蹭你网呢。所以，不是慢，是没有优化。 8 https 耗性能吗 ? 答案是，握手的时候耗，建好连接之后就不太耗了。按照目前加密强度的计算开销，服务器支撑握手性能会下降 6-8 倍，但是如果建立好连接之后，服务器就几乎可能撑住打满网卡的 https 流量了。所以连接复用率的提升和计算性能的优化都是重点。可以阅读《大型网站的 HTTPS 实践（三）-- 基于协议和配置的优化》 9 劫持有些什么样的途经 ? 你的电脑，你设置的 dns，你的浏览器，你用的网络，都有可能被劫持。简单和大家介绍下运营商的内容劫持是如何进行的，运营商会分析你的网络请求，它可以先于网站回包，也能修改数据包的内容。所以它可以让你跳转一次，在网址上加上小尾巴，也能在你访问的页面弹出小广告。感兴趣的话，还可以通过这篇文章看看你的电脑如何被 lsp 劫持的《暗云木马》 10 https 解决了所有劫持问题吗？ 俗话说有终有始，我们来说一说文章开始说的浏览器上的绿色标记。它标志着这个安全连接可信赖的级别。绿色通常是好的，黄色则是说明有些不安全，例如在 https 的页面中加载了 http 的资源，这样 http 的资源还是有被劫持的风险。其实客户端，局域网的风险也很大，恶意插件，木马可以做很多事情，你使用的路由器，DNS 也比较脆弱。如果某个大型网站被标记为了红色，那你就更要小心了 (当然也可能是某个猴子忘记了续费替换证书，导致证书过期了)，你有可能遭受了 ssl 劫持 (中间人攻击的一种)，特别是遇到如下图提示的时候（访问一些自己签名的站点也会有类似的提示）。中间人攻击还有其他种类的，比如代理你的通信让你退化 http, 还可以利用注入根证书，可以让你浏览器还是绿色的标记，就问你怕不怕？ 还是那句话，没有绝对的安全，但是我们可以尽量降低风险。https 能够在绝大部分情况下保证互联网访问数据传输的安全，这是目前我们力所能及的工作。 "},"seo/search-engine-principle.html":{"url":"seo/search-engine-principle.html","title":"百度搜索引擎工作原理","keywords":"","body":"百度搜索引擎工作原理 Spider 抓取系统的基本框架 互联网信息爆发式增长，如何有效的获取并利用这些信息是搜索引擎工作中的首要环节。数据抓取系统作为整个搜索系统中的上游，主要负责互联网信息的搜集、保存、更新环节，它像蜘蛛一样在网络间爬来爬去，因此通常会被叫做 “spider”。例如我们常用的几家通用搜索引擎蜘蛛被称为：Baiduspdier、Googlebot、Sogou Web Spider 等。 Spider 抓取系统是搜索引擎数据来源的重要保证，如果把 web 理解为一个有向图，那么 spider 的工作过程可以认为是对这个有向图的遍历。从一些重要的种子 URL 开始，通过页面上的超链接关系，不断的发现新 URL 并抓取，尽最大可能抓取到更多的有价值网页。对于类似百度这样的大型 spider 系统，因为每时 每刻都存在网页被修改、删除或出现新的超链接的可能，因此，还要对 spider 过去抓取过的页面保持更新，维护一个 URL 库和页面库。 下图为 spider 抓取系统的基本框架图，其中包括链接存储系统、链接选取系统、dns 解析服务系统、抓取调度系统、网页分析系统、链接提取系统、链接分析系统、网页存储系统。Baiduspider 即是通过这种系统的通力合作完成对互联网页面的抓取工作。 Baiduspider 主要抓取策略类型 上图看似简单，但其实 Baiduspider 在抓取过程中面对的是一个超级复杂的网络环境，为了使系统可以抓取到尽可能多的有价值资源并保持系统及实际环境中页面的一致性同时不给网站体验造成压力，会设计多种复杂的抓取策略。以下做简单介绍： 1. 抓取友好性 互联网资源庞大的数量级，这就要求抓取系统尽可能的高效利用带宽，在有限的硬件和带宽资源下尽可能多的抓取到有价值资源。这就造成了另一个问题，耗费被抓网站的带宽造成访问压力，如果程度过大将直接影响被抓网站的正常用户访问行为。因此，在抓取过程中就要进行一定的抓取压力控制，达到既不影响网站的正常用户访问又能尽量多的抓取到有价值资源的目的。 通常情况下，最基本的是基于 ip 的压力控制。这是因为如果基于域名，可能存在一个域名对多个 ip（很多大网站）或多个域名对应同一个 ip（小网站共享 ip）的问题。实际中，往往根据 ip 及域名的多种条件进行压力调配控制。同时，站长平台也推出了压力反馈工具，站长可以人工调配对自己网站的抓取压力，这时百度 spider 将优先按照站长的要求进行抓取压力控制。 对同一个站点的抓取速度控制一般分为两类：其一，一段时间内的抓取频率；其二，一段时间内的抓取流量。同一站点不同的时间抓取速度也会不同，例如夜深人静月黑风高时候抓取的可能就会快一些，也视具体站点类型而定，主要思想是错开正常用户访问高峰，不断的调整。对于不同站点，也需要不同的抓取速度。 2. 常用抓取返回码示意 简单介绍几种百度支持的返回码： 最常见的 404 代表 “NOT FOUND”，认为网页已经失效，通常将在库中删除，同时短期内如果 spider 再次发现这条 url 也不会抓取； 503 代表 “Service Unavailable”，认为网页临时不可访问，通常网站临时关闭，带宽有限等会产生这种情况。对于网页返回 503 状态码，百度 spider 不会把这条 url 直接删除，同时短期内将会反复访问几次，如果网页已恢复，则正常抓取；如果继续返回 503，那么这条 url 仍会被认为是失效链接，从库中删除。 403 代表 “Forbidden”，认为网页目前禁止访问。如果是新 url，spider 暂时不抓取，短期内同样会反复访问几次；如果是已收录 url，不会直接删除，短期内同样反复访问几次。如果网页正常访问，则正常抓取；如果仍然禁止访问，那么这条 url 也会被认为是失效链接，从库中删除。 301 代表是 “Moved Permanently”，认为网页重定向至新 url。当遇到站点迁移、域名更换、站点改版的情况时，我们推荐使用 301 返回码，同时使用站长平台网站改版工具，以减少改版对网站流量造成的损失。 3. 多种 url 重定向的识别 互联网中一部分网页因为各种各样的原因存在 url 重定向状态，为了对这部分资源正常抓取，就要求 spider 对 url 重定向进行识别判断，同时防止作弊行为。重定向可分为三类：http 30x 重定向、meta refresh 重定向和 js 重定向。另外，百度也支持 Canonical 标签，在效果上可以认为也是一种间接的重定向。 4. 抓取优先级调配 由于互联网资源规模的巨大以及迅速的变化，对于搜索引擎来说全部抓取到并合理的更新保持一致性几乎是不可能的事情，因此这就要求抓取系统设计一套合理的抓取优先级调配策略。主要包括：深度优先遍历策略、宽度优先遍历策略、pr 优先策略、反链策略、社会化分享指导策略等等。每个策略各有优劣，在实际情况中往往是多种策略结合使用以达到最优的抓取效果。 5. 重复 url 的过滤 spider 在抓取过程中需要判断一个页面是否已经抓取过了，如果还没有抓取再进行抓取网页的行为并放在已抓取网址集合中。判断是否已经抓取其中涉及到最核心的是快速查找并对比，同时涉及到 url 归一化识别，例如一个 url 中包含大量无效参数而实际是同一个页面，这将视为同一个 url 来对待。 6. 暗网数据的获取 互联网中存在着大量的搜索引擎暂时无法抓取到的数据，被称为暗网数据。一方面，很多网站的大量数据是存在于网络数据库中，spider 难以采用抓取网页的方式获得完整内容；另一方面，由于网络环境、网站本身不符合规范、孤岛等等问题，也会造成搜索引擎无法抓取。目前来说，对于暗网数据的获取主要思路仍然是通过开放平台采用数据提交的方式来解决，例如 “百度站长平台”“百度开放平台” 等等。 7. 抓取反作弊 spider 在抓取过程中往往会遇到所谓抓取黑洞或者面临大量低质量页面的困扰，这就要求抓取系统中同样需要设计一套完善的抓取反作弊系统。例如分析 url 特征、分析页面大小及内容、分析站点规模对应抓取规模等等。 Baiduspider 抓取过程中涉及的网络协议 刚才提到百度搜索引擎会设计复杂的抓取策略，其实搜索引擎与资源提供者之间存在相互依赖的关系，其中搜索引擎需要站长为其提供资源，否则搜索引擎就无法满足用户检索需求；而站长需要通过搜索引擎将自己的 内容推广出去获取更多的受众。spider 抓取系统直接涉及互联网资源提供者的利益，为了使搜素引擎与站长能够达到双赢，在抓取过程中双方必须遵守一定的 规范，以便于双方的数据处理及对接。这种过程中遵守的规范也就是日常中我们所说的一些网络协议。 以下简单列举： http 协议：超文本传输协议，是互联网上应用最为广泛的一种网络协议，客户端和服务器端请求和应答的标准。客户端一般情况是指终端用户，服务器端即指网 站。终端用户通过浏览器、蜘蛛等向服务器指定端口发送 http 请求。发送 http 请求会返回对应的 httpheader 信息，可以看到包括是否成功、服务 器类型、网页最近更新时间等内容。 https 协议：实际是加密版 http，一种更加安全的数据传输协议。 UA 属性：UA 即 user-agent，是 http 协议中的一个属性，代表了终端的身份，向服务器端表明我是谁来干嘛，进而服务器端可以根据不同的身份来做出不同的反馈结果。 robots 协议：robots.txt 是搜索引擎访问一个网站时要访问的第一个文件，用以来确定哪些是被允许抓取的哪些是被禁止抓取的。 robots.txt 必须放在网站根目录下，且文件名要小写。详细的 robots.txt 写法可参考 http://www.robotstxt.org 。百度严格按照 robots 协议执行，另外，同样支持网页内容中添加的名为 robots 的 meta 标 签，index、follow、nofollow 等指令。 Baiduspider 抓取频次原则及调整方法 Baiduspider 根据上述网站设置的协议对站点页面进行抓取，但是不可能做到对所有站点一视同仁，会综合考虑站点实际情况确定一个抓取配额，每天定量抓取站点内容，即我们常说的抓取频次。那么百度搜索引擎是根据什么指标来确定对一个网站的抓取频次的呢，主要指标有四个： 网站更新频率：更新快多来，更新慢少来，直接影响 Baiduspider 的来访频率 网站更新质量：更新频率提高了，仅仅是吸引了 Baiduspier 的注意，Baiduspider 对质量是有严格要求的，如果网站每天更新出的大量内容都被 Baiduspider 判定为低质页面，依然没有意义。 连通度：网站应该安全稳定、对 Baiduspider 保持畅通，经常给 Baiduspider 吃闭门羹可不是好事情 站点评价：百度搜索引擎对每个站点都会有一个评价，且这个评价会根据站点情况不断变化，是百度搜索引擎对站点的一个基础打分（绝非外界所说的百度权重），是百度内部一个非常机密的数据。站点评级从不独立使用，会配合其它因子和阈值一起共同影响对网站的抓取和排序。 抓取频次间接决定着网站有多少页面有可能被建库收录，如此重要的数值如果不符合站长预期该如何调整呢？百度站长平台提供了抓取频次工具（http://zhanzhang.baidu.com/pressure/index），并已完成多次升级。该工具除了提供抓取统计数据外，还提供 “频次调整” 功能，站长根据实际情况向百度站长平台提出希望 Baiduspider 增加来访或减少来访的请求，工具会根据站长的意愿和实际情况进行调整。 造成 Baiduspider 抓取异常的原因 有一些网页，内容优质，用户也可以正常访问，但是 Baiduspider 却无法正常访问并抓取，造成搜索结果覆盖率缺失，对百度搜索引擎对站点都是一种损失，百度把这种情况叫 “抓取异常”。对于大量内容无法正常抓取的网站，百度搜索引擎会认为网站存在用户体验上的缺陷，并降低对网站的评价，在抓取、索引、排序上都会受到一定程度的负面影响，最终影响到网站从百度获取的流量。 下面向站长介绍一些常见的抓取异常原因: 1. 服务器连接异常 服务器连接异常会有两种情况：一种是站点不稳定，Baiduspider 尝试连接您网站的服务器时出现暂时无法连接的情况；一种是 Baiduspider 一直无法连接上您网站的服务器。造成服务器连接异常的原因通常是您的网站服务器过大，超负荷运转。也有可能是您的网站运行不正常，请检查网站的 web 服务器（如 apache、iis）是否安装且正常运行，并使用浏览器检查主要页面能否正常访问。您的网站和主机还可能阻止了 Baiduspider 的访问，您需要检查网站和主机的防火墙。 2. 网络运营商异常 网络运营商分电信和联通两种，Baiduspider 通过电信或网通无法访问您的网站。如果出现这种情况，您需要与网络服务运营商进行联系，或者购买拥有双线服务的空间或者购买 cdn 服务。 3. DNS 异常 当 Baiduspider 无法解析您网站的 IP 时，会出现 DNS 异常。可能是您的网站 IP 地址错误，或者域名服务商把 Baiduspider 封禁。请使用 WHOIS 或者 host 查询自己网站 IP 地址是否正确且可解析，如果不正确或无法解析，请与域名注册商联系，更新您的 IP 地址。 4. IP 封禁 IP 封禁为：限制网络的出口 IP 地址，禁止该 IP 段的使用者进行内容访问，在这里特指封禁了 BaiduspiderIP。当您的网站不希望 Baiduspider 访问时，才需要该设置，如果您希望 Baiduspider 访问您的网站，请检查相关设置中是否误添加了 BaiduspiderIP。也有可能是您网站所在的空间服务商把百度 IP 进行了封禁，这时您需要联系服务商更改设置。 5. UA 封禁 UA 即为用户代理（User-Agent），服务器通过 UA 识别访问者的身份。当网站针对指定 UA 的访问，返回异常页面（如 403，500）或跳转到其他页面的情况，即为 UA 封禁。当您的网站不希望 Baiduspider 访问时，才需要该设置，如果您希望 Baiduspider 访问您的网站，useragent 相关的设置中是否有 Baiduspider UA，并及时修改。 6. 死链 页面已经无效，无法对用户提供任何有价值信息的页面就是死链接，包括协议死链和内容死链两种形式: 协议死链：页面的 TCP 协议状态 /HTTP 协议状态明确表示的死链，常见的如 404、403、503 状态等。 内容死链：服务器返回状态是正常的，但内容已经变更为不存在、已删除或需要权限等与原内容无关的信息页面。对于死链，我们建议站点使用协议死链，并通过百度站长平台--死链工具向百度提交，以便百度更快地发现死链，减少死链对用户以及搜索引擎造成的负面影响。 7. 异常跳转 将网络请求重新指向其他位置即为跳转。异常跳转指的是以下几种情况： 当前该页面为无效页面（内容已删除、死链等），直接跳转到前一目录或者首页，百度建议站长将该无效页面的入口超链接删除掉 跳转到出错或者无效页面 注意：对于长时间跳转到其他域名的情况，如网站更换域名，百度建议使用 301 跳转协议进行设置。 其他异常： 针对百度 refer 的异常：网页针对来自百度的 refer 返回不同于正常内容的行为。 针对百度 ua 的异常：网页对百度 UA 返回不同于页面原内容的行为。 JS 跳转异常：网页加载了百度无法识别的 JS 跳转代码，使得用户通过搜索结果进入页面后发生了跳转的情况。 压力过大引起的偶然封禁：百度会根据站点的规模、访问量等信息，自动设定一个合理的抓取压力。但是在异常情况下，如压力控制失常时，服务器会根据自身负荷进行保护性的偶然封禁。这种情况下，请在返回码中返回 503(其含义是 “Service Unavailable”)，这样 Baiduspider 会过段时间再来尝试抓取这个链接，如果网站已空闲，则会被成功抓取。 新链接重要程度判断 好啦，上面我们说了影响 Baiduspider 正常抓取的原因，下面就要说说 Baiduspider 的一些判断原则了。在建库环节前，Baiduspider 会对页面进行初步内容分析和链接分析，通过内容分析决定该网页是否需要建索引库，通过链接分析发现更多网页，再对更多网页进行抓取——分析——是否建库 & 发现新链接的流程。理论上，Baiduspider 会将新页面上所有能 “看到” 的链接都抓取回来，那么面对众多新链接，Baiduspider 根据什么判断哪个更重要呢？两方面： 第一，对用户的价值 内容独特，百度搜索引擎喜欢 unique 的内容 主体突出，切不要出现网页主体内容不突出而被搜索引擎误判为空短页面不抓取 内容丰富 广告适当 第二，链接重要程度 目录层级——浅层优先 链接在站内的受欢迎程度 百度优先建重要库的原则 Baiduspider 抓了多少页面并不是最重要的，重要的是有多少页面被建索引库，即我们常说的 “建库”。众所周知，搜索引擎的索引库是分层级的，优质的网页会被分配到重要索引库，普通网页会待在普通库，再差一些的网页会被分配到低级库去当补充材料。目前 60% 的检索需求只调用重要索引库即可满足，这也就解释了为什么有些网站的收录量超高流量却一直不理想。 那么，哪些网页可以进入优质索引库呢。其实总的原则就是一个：对用户的价值。包括却不仅于： 有时效性且有价值的页面：在这里，时效性和价值是并列关系，缺一不可。有些站点为了产生时效性内容页面做了大量采集工作，产生了一堆无价值面页，也是百度不愿看到的 . 内容优质的专题页面：专题页面的内容不一定完全是原创的，即可以很好地把各方内容整合在一起，或者增加一些新鲜的内容，比如观点和评论，给用户更丰富全面的内容。 高价值原创内容页面：百度把原创定义为花费一定成本、大量经验积累提取后形成的文章。千万不要再问我们伪原创是不是原创。 重要个人页面：这里仅举一个例子，科比在新浪微博开户了，即使他不经常更新，但对于百度来说，它仍然是一个极重要的页面。 哪些网页无法建入索引库 上述优质网页进了索引库，那其实互联网上大部分网站根本没有被百度收录。并非是百度没有发现他们，而是在建库前的筛选环节被过滤掉了。那怎样的网页在最初环节就被过滤掉了呢： 重复内容的网页：互联网上已有的内容，百度必然没有必要再收录。 主体内容空短的网页 有些内容使用了百度 spider 无法解析的技术，如 JS、AJAX 等，虽然用户访问能看到丰富的内容，依然会被搜索引擎抛弃 加载速度过慢的网页，也有可能被当作空短页面处理，注意广告加载时间算在网页整体加载时间内。 很多主体不突出的网页即使被抓取回来也会在这个环节被抛弃。 部分作弊网页 "},"seo/search-engine-optimization-guide.html":{"url":"seo/search-engine-optimization-guide.html","title":"百度搜索引擎优化指南2.0","keywords":"","body":"百度搜索引擎优化指南 2.0 前言 根据 DCCI2010 年中国互联网站长生存与发展状况调查的数据显示，中国互联网站长月收入在 500 元以下以及无收入的比例超过 50%，主要盈利模式仍以广告为主，大多数的互联网创业者面对着巨大的生存压力，发展状况令人担忧。如何更快更好的改变他们当前面临的困境，成为关系到互联网整个生态圈长足发展的重要议题。而针对互联网创业者，网站流量更是关乎于网站成败的关键。互联网创业者俱乐部是由百度发起成立，旨在帮助互联网创业者健康发展、扶持互联网创新力量、拓展互联网创业者合作与发展空间；为热爱互联网，并有志投身互联网的创业者搭建的非盈利平台。而首次发布的《搜索引擎优化指南》为的就是帮助互联网创业者在创业初期就步入正轨，用更为合理、科学的方式增加自身网站在搜索引擎中的收录数，进而提升流量，为未来快速、健康发展奠定坚实的基础。通过本指南，互联网创业者将获得明确、正规的搜索引擎优化标准，合理、可持续提升网站流量，获得长久、稳定的发展，并促进行业的良性发展。 搜索引擎优化（Search engine optimization，简称 SEO），指为了提升网页在搜索引擎自然搜索结果中（非商业性推广结果）的收录数量以及排序位置而做的优化行为，这一行为的目的，是为了从搜索引擎中获得更多的免费流量，以及更好的展现形象。而 SEM（Search engine marketing，搜索引擎营销），则既包括了 SEO，也包括了付费的商业推广优化。 SEO 自从 1997 年左右出现以来，逐渐分化成两类 SEO 行为：一类被称为 白帽 SEO，这类 SEO 起到了改良和规范网站设计的作用，使之对搜索引擎和用户更加友好，并从中获取更多合理的流量。搜索引擎鼓励和支持 白帽 SEO。另一类被称为 黑帽 SEO，这类 SEO 行为利用和放大搜索引擎的策略缺陷（实际上完美的系统是不存在的）获取更多用户访问量，而这些更多的访问量，是以伤害用户体验为代价的，所以，面对后一种 SEO 行为，搜索引擎会通过一些策略进行遏制。 搜索引擎与 SEO 行为间是一种良性的共生关系，比如很多优质的网站是用 Flash 或者 Ajax 做的，搜索引擎就无法很好的爬取和索引。建站者在了解了 SEO 的一些基本原理后，可以通过对网站的合理优化，使这些优质资源更好的发挥其检索效果，改善用户的搜索体验。同时，对于中国这样的新兴市场，传统的中小企业对于如何触网，如何做互联网营销，并无多少经验，在广大的互联网创业者中，对于如何 SEO 也充斥着矛盾的舆论和猜想。让更多人了解搜索引擎的工作机制，引导广泛合理的 SEO 行为，让认真做原创优质内容的创业者得到更多流量，令抄袭抓取别人内容的建站行为得到警惕，是百度作为中文搜索领域的领导者应有的责任和义务，只有这样才能有效支持互联网创新力量，使互联网生态圈得到更加健康有序的发展。 需要指出的是，此次发布的《搜索引擎优化指南》，虽然听取了部分站长的意见和建议，但仍有很大的提升空间。未来，我们会收集更多的建设性意见，来不断的完善这一指南。 "},"seo/mobile-friendly-standard.html":{"url":"seo/mobile-friendly-standard.html","title":"百度搜索Mobile Friendly（移动友好度）标准V1.0","keywords":"","body":"百度搜索Mobile Friendly（移动友好度）标准V1.0 百度搜索是全球最大的中文搜索引擎。在移动互联时代，百度每天响应移动搜索请求高达几十亿次，导向互联网的流量几十亿量级，且快速增长。面对移动用户的迅猛崛起，站长们纷纷涌入移动化建设的浪潮中。百度秉承用户体验至上的理念，以移动用户体验为导向，发布移动友好度标准，旨在帮助站长建立适合移动设备应用的网站，为网站移动化建设提供明确的方向。本次拟先发布移动友好度标准V1.0版，后续会不断收集站长反馈并尽快发布移动友好度开发指南，以更好地规范并指导移动建站。 移动友好度概述 当用户在手机上点按百度搜索结果时，除了搜索结果对需求的满足程度外，搜索结果的加载时间、页面浏览体验、资源或功能的易用性、页面是否符合移动端适配等，都影响移动用户体验的满意度。百度致力于帮助移动用户获得更好的移动页浏览体验，移动友好度是一个重要的评价因子。 这份标准旨在告诉广大站长，怎样的移动页是受用户欢迎的，不仅针对搜索引擎，百度更鼓励站长从用户角度来建设网页。 面向用户体验的网站建设：页面可读性 不可访问的页面会被直接归入垃圾页面，百度不会浪费资源对垃圾页面进行移动友好度评估。页面可读是衡量移动友好度的基础。这里的页面可读主要指用户可读，即用户能够看见、看清并看懂页面。页面可读性包含如下维度：页面加载速度体验、页面结构、页面浏览体验。 【页面加载速度体验】 移动互联网上，网页的加载速度对用户体验的影响日趋明显。百度用户体验部研究表明，用户期望且能够接受的页面加载时间在3秒以内。若页面的加载时间超过5秒，78%的用户会失去耐心而选择离开。页面加载速度是百度搜索中一个重要的排序因素，百度再次建议站长对这方面进行专项优化。 【页面结构】 一个结构优质的页面，要让用户第一眼看到页面的主要内容，获取页面主体信息时没有多余的干扰，快速找到所需。用户能够通过页面布局结构，快速了解页面各模块的主要内容。要构建一个结构优质的移动页需关注2点： 基本地，页面能够根据屏幕调整内容大小，用户不需要左右滚动，也不需要进行缩放操作就能清晰辨识网页的内容。 其次，页面主体位于首屏且中心的位置，其他相关度低的内容对页面主体无干扰作用。百度会严厉打击应用恶意弹窗/浮层的行为。视对用户体验造成伤害程度的大小，在结果排序上会对以下情况减分：广告遮盖主体、广告动态抢夺用户视线、广告穿插主体等。 页面结构相关示例如下： 【页面浏览体验】 页面浏览体验和页面结构密切相关：页面结构差，浏览体验无从谈起。页面结构优质，想给用户更好的浏览体验，首先注意： 页面主体中的文本内容和背景色应有明显的区分度； 页面主体中的文本内容应段落分明，排版精良； 百度用户体验部对移动页浏览体验的研究成果： 主体内容含文本段落时，正文字号推荐14px，行间距推荐（0.42~0.6）字号，正文字号不小于10px，行间距不小于0.2字号； 主体内容含多图时，除图片质量外，应设置图片宽度一致位置统一； 主体内容含多个文字链时，文字链字号推荐14px或16px：字号为14px时，纵向间距推荐13px；字号为16px时，纵向间距推荐14px；文字链整体可点区域不小于40px； 主体内容中的其他可点区域，宽度和高度应大于40px； 此外，需注意交互一致性，同一页面不应使用相同手势完成不同功能。 页面浏览体验相关示例如下： 面向用户体验的网站建设：资源易用性 【资源易用性】 按照页面主体内容载体的不同，资源易用性的标准会有较大的不同： 首页或索引页：页面提供的导航链接应清晰可点，页面推荐的内容应清晰有效； 文本页面：页面提供的内容应清晰完整，有精良的排版。文本页面包括文章页. 问答页. 论坛页等。 Flash：Flash 是移动设备上不常用的资源形式，应避免使用； 音/视频页：音/视频应能够直接播放，且资源清晰优质。百度严厉打击欺诈性下载播放器的行为； APP下载：APP应提供直接下载，且下载的为最佳版本；百度严厉打击欺诈性下载手机助手和应用市场的行为； 文档页：应提供可直接阅读的文档，且文档阅读体验好；请注意，将文档资源转化为图片资源的的方式，不仅影响用户体验，对搜索引擎也不友好，应避免。 服务页&功能页：提供的服务或功能应易用好用，下一部分详细说明。 面向用户体验的网站建设：功能易用性 【功能易用性】 按照页面主体功能的不同，功能易用性的考量区别如下： 商品页：页面应提供完整的商品信息和有效地购买路径。 搜索结果页：页面罗列出的搜索结果应与搜索词密切相关。 表单页：页面应提供完整有效的功能。表单页主要指注册页、登陆页、信息提交页等。 资源及功能易用性相关示例如下： 图1.资源易用性-Flash不可用 面向用户体验的网站建设：体验增益性 这个维度属于增益项，只有当页面在可读性和资源及功能易用性上表现较好时，百度排序时才会考虑体验增益性，并给予额外的优待： 提供访问路径上的增益，例如页面提供有效的导航或面包屑（My post），能够去往上一级或下一级页面； 生活服务类网站，提供效率上的增益，例如电话可拨打、地址可定位； 查询类网站，提供输入方式上的增益，例如支持语音输入、图像输入、扫码功能等； 阅读类网站，提供体验增益，例如夜间模式等。 体验增益性相关示例如下： 在移动端，APP自调起未判断用户需求状态即切断搜索流程，破坏了搜索体验的完整性和流畅性，使用户对点击搜索结果后的效果没有预期，建议站长使用Applink服务并安装相应的SDK，保障用户搜索体验，并实现Html5和APP内容的无缝打通。近年来服务类O2O产品大量涌现，百度建议只有APP或微信服务号的商户将Html5资源提交给百度，实现更好的收益。 体验提升，标准先行。百度搜索会依据此标准在移动端推出移动友好度算法，将移动友好度作为移动搜索排名的一项重要参考因素。同时会不定期抽样移动页进行人工评估，评估后的结果会返回技术部门进行机器学习。百度搜索会继续完善标准，优化算法，不遗余力地提升移动用户体验，并更好地为移动站长服务。 "},"seo/baidu-mobile-search-optimization-guide2.0.html":{"url":"seo/baidu-mobile-search-optimization-guide2.0.html","title":"百度移动搜索优化指南2.0","keywords":"","body":"百度移动搜索优化指南 2.0 前期准备工作 【域名】 　　与 PC 网站一样，域名是用户对一个网站的第一印象。一个好的移动域名，不仅容易记忆、易于输入，还能方便用户向其他人推荐。域名应尽量简短易懂，越短的域名记忆成本越低，越容易理解的域名能让用户更直观了解网站主旨。移动站域名建议多采用m.a.com/3g.a.com/wap.a.com等，避免使用过于复杂或技术性的形式，例如 adcbxxx.a.com/html5.a.com 等。 【服务器】 　　选择正规空间服务商，避免与大量垃圾网站共用 IP，保证网站访问速度和稳定性。其他这里不再赘述，默认有 PC 网站基础。 【网站语言】 　　根据终端以及技术的发展，我们强烈建议使用 html5 作为移动站建站语言，并且根据不同终端机型进行自动适配。 "},"seo/network-optimization.html":{"url":"seo/network-optimization.html","title":"内容建设","keywords":"","body":"网站优化 知名站点优化注意事项 百度需要优质站点为搜索引擎数据库源源不断地输入物料，同时优质站点也需要从百度获得搜索引擎用户，并将这些搜索引擎用户转化为自己的用户。知名站点可以视为优质站点的一部分，是指已经有较高用户知名度的网站。那么，站点越是知名，就越应从长远考虑，以用户体验为重，积极、合理的进行网站优化，远离作弊和恶意 SEO 行为，建立与百度更加稳固的合作关系。 但我们经常可以遗憾地看到一些知名站点使用了不够合理的内容建设方式，比如：大量不同内容的页面均使用同一标题；通过图片的方式展现网页中的重要信息（新闻、联系电话等）；重要页面通过 flash 建设，未使用文字说明等。 类似的方式，都会使搜索引擎对网站内容的理解造成困难，最终影响网站在搜索引擎中的表现。我们建议您采取对搜索引擎友好的方式进行网站建设，具体内容可参考《百度搜索引擎优化指南》以及《百度搜索引擎网页质量白皮书》。 知名站点对搜索引擎，乃至整个互联网欢迎的影响通常是巨大的，若使用恶意的作弊行为，其产生的影响也就越坏。例如： 出售首页的友情链接。 大量采集其他网站的内容，原创内容比例极低，甚至大量采集网站主题无关的内容，或机器批量生成没有价值的页面。 网站被大量广告内容占领。 知名站点一旦有恶意的作弊行为，所产生的恶劣影响将是巨大的，有可能会受到搜索引擎更为严格的惩罚。知名站点更应从长远考虑，提升用户口碑，用高质量的内容赢得用户青睐。不应为了短期的流量，不顾用户的体验，做有损品牌形象的事情。 此外，知名站点也是各路作弊者的 “众矢之的”，容易被恶意利用。站点越知名，被利用的价值就越大。我们建议您建立防范意识，避免由于被作弊者利用引起的不必要风险。 您的站点是否足够安全？当网站被黑客攻击时，是否有足够的应对机制？ 由用户贡献内容的功能，是否做好了应对作弊的措施？ 对于不想让搜索引擎收录的内容，是否在 robot.txt 中清晰指明？ 百度索引量变化追查投诉方法 相信无论站点知名，都对百度索引量数据非常敏感，毕竟索引量（或称收录量）属于网站流量的基石，盯着百度站长平台提供的索引量工具查看数据应该是很多站长每天的例行工作。理论上说，索引量发生波动属于正常，但也不应掉以轻心。可以通过以下流程图来进行问题排查。 关于申诉，除了再一次播报申诉地址（http://zhanzhang.baidu.com/feedback）外，我们给各位站长提个醒，在撰写申诉内容时应该尽量将问题描述具体，引用网友的经验：“网站索引量异常，可以使用百度站长平台的索引量查询工具，一级一级的遍历一下自己网站的主要子域名或目录，以确定到底是哪个子域名或目录的索引量出现了异常。”“每个频道选取一些页面，在百度网页搜索中直接搜索这样页面的 URL，以定位被删除快照网页的最小范围。”“然后在投诉内容中明确给出 '病体' 的 URL，并附上相应的数据变动截图。” 这样才便于处理投诉的百度工作人员快速寻找问题症结。 另外需要注意的是，站长们往往只有在索引量下降时心惊肉跳一把，在百度内部人员看来，索引量无缘无故大幅增长更应该警惕，应该检查一下： 会不会是网站被黑客攻击后增加了大量垃圾网页 会不会是 Robost 协议出了问题，导致大批保密页面被百度抓取 大幅增加的 url 会不会占用有限的抓取配额，导致重要优质内容未被抓取 "},"seo/website-development.html":{"url":"seo/website-development.html","title":"网站建设","keywords":"","body":"网站建设 如何正确识别 Baiduspider 移动 ua 新版移动 ua: Mozilla/5.0 (Linux;u;Android 4.2.2;zh-cn;) AppleWebKit/534.46 (KHTML,like Gecko) Version/5.1 Mobile Safari/10600.6.3 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html) PC ua: Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html） 之前通过 “+http://www.baidu.com/search/spider.html” 进行识别的网站请注意！您需要修改识别方式，新的正确的识别 Baiduspider 移动 ua 的方法如下： 通过关键词 “Android” 或者 “Mobile” 来进行识别，判断为移动访问或者抓取。 通过关键词 “Baiduspider/2.0”, 判断为百度爬虫。 另外需要强调的是，对于 robots 封禁，如果封禁的 agent 是 Baiduspider，会对 PC 和移动同时生效。即，无论是 PC 还是移动 Baiduspider，都不会对封禁对象进行抓取。之所以要强调这一点，是发现有些代码适配站点（同一个 url，PC ua 打开的时候是 PC 页，移动 ua 打开的时候是移动页），想通过设置 robots 的 agent 封禁达到只让移动 Baiduspider 抓取的目的，但由于 PC 和移动 Baiduspider 的 agent 都是 Baiduspider, 这种方法是非常不可取的。 如何识别百度蜘蛛 百度蜘蛛对于站长来说可谓上宾，可是我们曾经遇到站长这样提问：我们如何判断疯狂抓我们网站内容的蜘蛛是不是百度的？其实站长可以通过 DNS 反查 IP 的方式判断某只 spider 是否来自百度搜索引擎。根据平台不同验证方法不同，如 linux/windows/os 三种平台下的验证方法分别如下： 在 linux 平台下，您可以使用 host ip 命令反解 ip 来判断是否来自 Baiduspider 的抓取。Baiduspider 的 hostname 以 .baidu.com 或 .baidu.jp 的格式命名，非 .baidu.com 或 .baidu.jp 即为冒充。 在 windows 平台或者 IBM OS/2 平台下，您可以使用 nslookup ip 命令反解 ip 来 判断是否来自 Baiduspider 的抓取。打开命令处理器 输入 nslookup xxx.xxx.xxx.xxx（IP 地 址）就能解析 ip， 来判断是否来自 Baiduspider 的抓取，Baiduspider 的 hostname 以 .baidu.com 或 .baidu.jp 的格式命名，非 .baidu.com 或 .baidu.jp 即为冒充。 在 mac os 平台下，您可以使用 dig 命令反解 ip 来 判断是否来自 Baiduspider 的抓取。打开命令处理器 输入 dig xxx.xxx.xxx.xxx（IP 地 址）就能解析 ip， 来判断是否来自 Baiduspider 的抓取，Baiduspider 的 hostname 以 .baidu.com 或 .baidu.jp 的格式命名，非 .baidu.com 或 .baidu.jp 即为冒充。 Baiduspider IP 是多少 即便很多站长知道了如何判断百度蜘蛛，仍然会不断地问 “百度蜘蛛 IP 是多少”。我们理解站长的意思，是想将百度蜘蛛所在 IP 加入白名单，只准白名单下 IP 对网站进行抓取，避免被采集等行为。 但我们不建议站长这样做。虽然百度蜘蛛的确有一个 IP 池，真实 IP 在这个 IP 池内切换，但是我们无法保证这个 IP 池整体不会发生变化。所以，我们建议站长勤看日志，发现恶意蜘蛛后放入黑名单，以保证百度的正常抓取。 同时，我们再次强调，通过 IP 来分辨百度蜘蛛的属性是非常可笑的事情，所谓的 “沙盒蜘蛛”“降权蜘蛛” 等等是从来都不存在的。 robots 写法 robots 是站点与 spider 沟通的重要渠道，站点通过 robots 文件声明该网站中不想被搜索引擎收录的部分或者指定搜索引擎只收录特定的部分。请注意，仅当您的网站包含不希望被搜索引擎收录的内容时，才需要使用 robots.txt 文件。如果您希望搜索引擎收录网站上所有内容，请勿建立 robots.txt 文件。 robots 文件往往放置于根目录下，包含一条或更多的记录，这些记录通过空行分开（以 CR,CR/NL, or NL 作为结束符），每一条记录的格式如下所示： : 在该文件中可以使用#进行注解，具体使用方法和 UNIX 中的惯例一样。该文件中的记录通常以一行或多行 User-agent 开始，后面加上若干 Disallow 和 Allow 行 , 详细情况如下： User-agent:该项的值用于描述搜索引擎 robot 的名字。在 \"robots.txt\" 文件中，如果有多条- User-agent 记录说明有多个 robot 会受到 \"robots.txt\" 的限制，对该文件来说，至少要有一条 User-agent 记录。如果该项的值设为 ，则对任何 robot 均有效，在 \"robots.txt\" 文件中，\"User-agent:\" 这样的记录只能有一条。如果在 \"robots.txt\" 文件中，加入 \"User-agent:SomeBot\" 和若干 Disallow、Allow 行，那么名为 \"SomeBot\" 只受到 \"User-agent:SomeBot\" 后面的 Disallow 和 Allow 行的限制。 Disallow:该项的值用于描述不希望被访问的一组 URL，这个值可以是一条完整的路径，也可以是路径的非空前缀，以 Disallow 项的值开头的 URL 不会被 robot 访问。例如 \"Disallow:/help\" 禁止 robot 访问 /help.html、/helpabc.html、/help/index.html，而 \"Disallow:/help/\" 则允许 robot 访问 /help.html、/helpabc.html，不能访问 /help/index.html。\"Disallow:\" 说明允许 robot 访问该网站的所有 url，在 \"/robots.txt\" 文件中，至少要有一条 Disallow 记录。如果 \"/robots.txt\" 不存在或者为空文件，则对于所有的搜索引擎 robot，该网站都是开放的。 Allow:该项的值用于描述希望被访问的一组 URL，与 Disallow 项相似，这个值可以是一条完整的路径，也可以是路径的前缀，以 Allow 项的值开头的 URL 是允许 robot 访问的。例如 \"Allow:/hibaidu\" 允许 robot 访问 /hibaidu.htm、/hibaiducom.html、/hibaidu/com.html。一个网站的所有 URL 默认是 Allow 的，所以 Allow 通常与 Disallow 搭配使用，实现允许访问一部分网页同时禁止访问其它所有 URL 的功能。 使用 \"*\"and\"$\"：Baiduspider 支持使用通配符 \"\" 和 \"$\" 来模糊匹配 url。 \"\" 匹配 0 或多个任意字符 \"$\" 匹配行结束符。 最后需要说明的是：百度会严格遵守 robots 的相关协议，请注意区分您不想被抓取或收录的目录的大小写，百度会对 robots 中所写的文件和您不想被抓取和收录的目录做精确匹配，否则 robots 协议无法生效。 robots 需求用法对应表 上面说了那么多理论，有没有简单对照表格告诉我，怎样的需求场景下应该怎样撰写 robots 文件？有的： 除 robots 外其它禁止百度收录的方法 Meta robots 标签是页面 head 部分标签的一种，也是一种用于禁止搜索引擎索引页面内容的指令。目前百度仅支持 nofollow 和 noarchive nofollow：禁止搜索引擎跟踪本页面上的链接如果您不想搜索引擎追踪此网页上的链接，且不传递链接的权重，请将此元标记置入网页的 部分：如果您不想百度追踪某一条特定链接，百度还支持更精确的控制，请将此标记直接写在某条链接上：signin要允许其他搜索引擎跟踪，但仅防止百度跟踪您网页的链接，请将此元标记置入网页的 部分： noarchive：禁止搜索引擎在搜索结果中显示网页快照要防止所有搜索引擎显示您网站的快照，请将此元标记置入网页的 部分：要允许其他搜索引擎显示快照，但仅防止百度显示，请使用以下标记：注：此标记只是禁止百度显示该网页的快照，百度会继续为网页建索引，并在搜索结果中显示网页摘要。 使用 robots 巧妙避免蜘蛛黑洞 对于百度搜索引擎来说，蜘蛛黑洞特指网站通过极低的成本制造出大量参数过多、内容类同但 url 不同的动态 URL ，就像一个无限循环的 “黑洞 “，将 spider 困住。spider 浪费了大量资源抓取的却是无效网页。 如很多网站都有筛选功能，通过筛选功能产生的网页经常会被搜索引擎大量抓取，而这其中很大一部分是检索价值低质的页面。如 “500-1000 之间价格的租房”，首先网站（包括现实中）上基本没有相关资源，其次站内用户和搜索引擎用户都没有这种检索习惯。这种网页被搜索引擎大量抓取，只能是占用网站宝贵的抓取配额，那么该如何避免这种情况呢？ 我们以北京美团网为例，看看美团网是如何利用 robots 巧妙避免这种蜘蛛黑洞的： 对于普通的筛选结果页，使用了静态链接，如： http://bj.meituan.com/category/zizhucan/weigongcun 同样是条件筛选结果页，当用户选择不同排序条件后，会生成带有不同参数的动态链接，而且即使是同一种排序条件（如：都是按销量降序排列），生成的参数也都是不同的。如： http://bj.meituan.com/category/zizhucan/weigongcun/hot?mtt=1.index%2Fpoi.0.0.i1afqhekhttp://bj.meituan.com/category/zizhucan/weigongcun/hot?mtt=1.index%2Fpoi.0.0.i1afqi5c对于美团网来说，只让搜索引擎抓取筛选结果页就可以了，而各种带参数的结果排序页面则通过 robots 规则拒绝提供给搜索引擎。在 robots.txt 的文件用法中有这样一条规则：Disallow: /? ，即禁止搜索引擎访问网站中所有的动态页面。美团网恰是通过这种方式，对 spider 优先展示高质量页面、屏蔽了低质量页面，为 spider 提供了更友好的网站结构，避免了黑洞的形成。 禁止百度图片搜索收录某些图片的方法 目前百度图片搜索也使用了与百度网页搜索相同的 spider，如果想禁止 Baiduspider 抓取网站上所有图片、禁止或允许 Baiduspider 抓取网站上的某种特定格式的图片文件可以通过设置 robots 实现：| | | --- | --- 禁止 Baiduspider 抓取网站上所有图片 , 仅允许抓取网页，禁止抓取任何图片。| User-agent: Baiduspider Disallow: /.jpg$; Disallow: /.jpeg$; Disallow: /.gif$; Disallow: /.png$; Disallow: /*.bmp$; 仅允许 Baiduspider 抓取网页和 .gif 格式图片 , 允许抓取网页和 gif 格式图片，不允许抓取其他格式图片| User-agent: Baiduspider; Allow: /.gif$; Disallow: /.jpg$; Disallow: /.jpeg$; Disallow: /.png$; Disallow: /*.bmp$; 仅禁止 Baiduspider 抓取 .jpg 格式图片|User-agent: Baiduspider;Disallow: /*.jpg$ 503 状态码的使用场景及与 404 的区别 Html 状态码也是网站与 spider 进行交流的渠道之一，会告诉 spider 网页目前的状态，spider 再来决定下一步该怎么做——抓 or 不抓 or 下次再来看看。对于 200 状态码大家已经很熟了，那么我们来介绍几种常见的失效网页使用的状态码： 【404】404 返回码的含义是 “NOT FOUND”，百度会认为网页已经失效，那么通常会从搜索结果中删除，并且短期内 spider 再次发现这条 url 也不会抓取。 【503】503 返回码的含义是 “Service Unavailable”，百度会认为该网页临时不可访问，通常网站临时关闭，带宽有限等会产生这种情况。对于网页返回 503，百度 spider 不会把这条 url 直接删除，短期内会再访问。届时如果网页已恢复，则正常抓取；如果继续返回 503，短期内还会反复访问几次。但是如果网页长期返回 503，那么这个 url 仍会被百度认为是失效链接，从搜索结果中删除。 【301】301 返回码的含义是 “Moved Permanently”，百度会认为网页当前跳转至新 url。当遇到站点迁移，域名更换、站点改版的情况时，推荐使用 301 返回码，尽量减少改版带来的流量损失。虽然百度 spider 现在对 301 跳转的响应周期较长，但我们还是推荐大家这么做。 一些网站把未建好的页面放到线上，并且这些页面在被百度抓取的时候返回码是 404，于是页面被百度抓取后会被当成死链直接删除。但是这个页面在第二天或者过几天就会建好，并上线到网站内展现出来，但是由于之前已经被百度当做死链删除，所以要再次等这些链接被发现，然后抓取才能展现，这样的操作最终会导致一些页面在一段时间内不会在百度出现。比如某个大型门户网站爆出大量新闻时效性很强的专题未被百度及时收录的问题，经检查验证后发现就是因为页面未建成就放置在线上，并返回 404 状态码，被百度当成死链删除，造成收录和流量损失。 对于这样的问题，建议网站使用 503 返回码，这样百度的抓取器接到 503 返回码后，会在隔一段时间再去访问这个地址，在您的页面建好上线后，就又能被重新抓取，保证页面及时被抓取和索引，避免未建好的内容设为 404 后带来不必要的收录和流量损失。 其它建议和总结： 如果百度 spider 对您的站点抓取压力过大，请尽量不要使用 404，同样建议返回 503。这样百度 spider 会过段时间再来尝试抓取这个链接，如果那个时间站点空闲，那它就会被成功抓取了 如果站点临时关闭或者页面暂未建好上线，当网页不能打开时以及没有建好时，不要立即返回 404，建议使用 503 状态。503 可以告知百度 spider 该页面临时不可访问，请过段时间再重试。 连通率为 0 的前因后果及预防补救措施 在百度 site 站点时，遇到 “连通率为 0” 说明 Baiduspider 在访问站点时发现了问题，该问题非常严重，如果 Baiduspider 持续一天时间无法访问有可能给站点带来灾难性的损失。 一、何谓连通率 　　既然为率，那么就是一个统计相关概念。我们不妨可以这样理解一个网站的连通率：百度蜘蛛在抓取网站页面的时候计算的一个概率，比如它抓取某网站中的 N 个页面，这 N 个页面都能打开并被蜘蛛成功抓取，这样的连通率就是 N/N=100%，反之蜘蛛抓取 N 个页面有 M 个页面能成功抓取，连通率就是 M/N 这里先说明几个常识： 不是网站被某算法惩罚而导致连通率为 0，也不是网站流量下降而导致连通率数值很低； 连通率代表着百度蜘蛛获取网站数据的情况，如果连通率数值很低或为 0，那么可能连维持网站的正常收录等都成问题； 假如网站被判定连通不了，那么如果让一个连通不了的站点在百度中获取排名，就会大大影响搜索用户的体验，于是百度就会采取对该站点有效引流的索引清除操作，一些站长就会发现自己的网站索引量突然大幅度下降，甚至索引被短暂清 0 了，接着就是网站百度搜索流量大幅度下降，这种突发性的事件又会造成站长们的揣测，以为自己的网站被百度惩罚了，其实站长们先应该注意的是站点的连通情况。 二、提取核心词 　　上述概念中，我们需要注意的几个词有：” 百度蜘蛛”； ” 在抓取”； ”N 个页面”； ”M 个页面”； ” 都能打开并被成功抓取”。 三、连通率很低或为 0 时 我们抓住上述 5 个提取出的核心词进行分析。 （一）百度蜘蛛 谨防百度蜘蛛访问被操控或禁止 由于百度蜘蛛的抓取量等原因对一些服务器造成压力，而这些网络商或网站运营方从某利益角度考虑，无意或有意的屏蔽百度蜘蛛，导致部分或全部百度蜘蛛 ip 获取不到网站数据； 一些网站运营方或网络商对蜘蛛进行区别对待，比如国外蜘蛛 ip、山东蜘蛛 ip、北京蜘蛛 ip 等返回不同的数据，移花接木的手段层出不穷，而这个过程很可能造成百度蜘蛛的异常行为或异常状态返回； 所以我们首先要确保百度蜘蛛的访问有没有被操控或禁止。 预防措施 完善联系方式获得百度站长工具消息提醒 虽然一些网站异常情况会收到百度站长工具消息提醒，然而我想告诉大家不是所有自己站点异常都可以寄希望于消息提醒的，自己心里要知道连发邮件都可能出现收不到的情况，更何况各种复杂的异常情况，因此不要出问题了只会问 “为什么站长工具没有给我发消息”。 利用抓取异常和抓取频次工具观察抓取情况↓抓取异常工具的网址异常情况 ↓抓取频次工具的抓取统计情况 ↓每个工具页面的右侧下方都有使用说明，如下 （3）抓取诊断工具是重中之重 为了保证网站对百度蜘蛛的稳定访问性能，日常我们需要养成定期抓取诊断的习惯，另外抓取诊断不是说光看看状态是否 “抓取成功” 就行了。有下面几步进行： 第一步：下拉选择分”pc”、 ” 移动” 进行抓取，”pc” 意味着一般意义上的电脑端访问诊断，” 移动” 则是如手机、平板等移动设备端访问诊断； 第二步：网站主要的引流页，如首页、详情页、内容页、专题页等都要进行抓取，另外可以重点几个时间段（比如网站高峰时间段）每天定期进行测试； 第三步：抓取失败了，点击” 抓取失败” 查看提示信息，如果自己没有技术能力解决问题，可以跟空间商进行沟通，然后向工具提交报错； 第四步：抓取成功后，也不能说就万事大吉了，还要点击” 抓取成功” 进去注意：提交网址、抓取网址、抓取 UA、网站 ip、下载时长、头部信息（服务器返回状态码、gzip 等相关信息）、网页源码是否都正常。 特别说明：有很多站长就光注意抓取成功，却不知网站 ip 可能并非自己的实际 ip，被网络商搞到哪里去了都不知道，还可能每隔一段时间都变。当发现 ip 有问题，及时跟网络商沟通，并在网站 ip 旁点击 “报错” 按钮，百度会更新网站 ip，但是切记不要 ip 变化频繁。此外当然还可能出现实际抓取网址、头部信息、网页源码等都不是自己本来设置的。 （二）在抓取 　　这是反映百度蜘蛛在抓取时的状态，百度的工具显示的数值肯定都是抓取后计算出来的数据，因此任何工具的连通率数据必定存在延迟性，另外这个计算过程也存在一定可能的错误，所以我们看到任何工具中关于连通率的数据，不要说 “我网站用抓取诊断等工具检查访问情况都好好的，为什么连通率还是 0”，因此除了上述建议的多抓取诊断测试外，自己可以加些监控网站连接状态的措施，笔者本人就曾经接收了不少关于网站连接不通的提醒。这时我会及时跟网络商沟通，然后及时用抓取诊断检查蜘蛛的情况。 （三）N 个页面与 M 个页面 　　这 N、M 个页面，可能百度蜘蛛很凑巧就赶上高峰的时候或者一些假死页面（执行时间较长，超过蜘蛛的耐心），那么 M 这个数值就会很低，统计概率的东西大家都懂的，那么这时网站的连通率依旧很低或为 0。因此若连通率为 0，我们还可以知道自己应该注意查看访问日志，及时清理死链，或者并发连接数增大，或者增加内存等等。 （四）都能打开并被成功抓取 这里主要注意 DNS 和空间的稳定性。 DNS 的问题 参考当心 dns 服务器不稳导致站点被屏 提醒大家注意的是现在不少云类 ns 服务器，这个过程中由于处理机制问题，包括回源障碍等等，较容易造成这个问题。另外国内大型服务商提供，比如你使用了 dnspod，并不代表你的 dns 就应该是 ok 的。有的站长存在着对大型服务商的错误认识，如 “新网的 dns 就是不可靠的，我都用百度云加速”。我们要明白廉价的东西质量都有一定局限性，所以需要自己检查 dns 解析情况，具体上网找找相关资料或平台，看看 dns 解析出的 ip 以及解析延迟情况等等。 空间的稳定性 很多人都会用超级 ping 了，这个不多说了。但是我还要告诉大家有下面两点需要注意： A、不良的网络商会对不同用户 ip 进行不同处理，自己可以用 vpn 等工具观察下不同地区 ip 段的网站访问情况与返回内容； B、空间的资源不足，内存、并发连接等等，当访问量很少的时候，自己察觉不出，需要提高访问量，增加连接时间。因此使用一些监控工具时，发现监控工具访问测试量巨大，或者被人刷流量时，你应该庆幸，而不是着急拒绝，因为你可以了解到自己的空间承压能力。 四、连通率问题处理完毕后 　　如果你的网站索引由于连通率而非惩罚原因被清理了，处理完毕问题，可以在抓取频次工具中提交增加抓取频次的请求，将抓取频次增加到一定额度（建议自己查看工具中对抓取频次的说明后再根据网站实际情况调整），然后增加自己的网站数据更新频率与质量，加强与百度的数据沟通（如 url 提交、sitemap 等等），很快就能恢复。相关工具展示如下（每个工具页面的右侧下方都有相关说明链接，可以点击去了解使用注意事项）： 抓取频次中的频次调整工具： 链接提交工具： 链接提交工具中的主动推送、sitemap、手动提交方式： https 站点如何做才能对百度友好 2015 年 5 月 25 日，百度站长平台发布公告，宣布全面放开对 https 站点的收录，https 站点不再需要做任何额外工作即可被百度抓收。采用了本文之前建议的 https 站点可以关闭 http 版，或者将 http 页面跳转到对应的 https 页面。 百度搜索引擎目前不主动抓取 https 网页，导致大部分 https 网页无法收录。但是如果网站一定要使用 https 加密协议的话该如何做到对百度搜索引擎友好呢。其实很简单： 为需要被百度搜索引擎收录的 https 页面制作 http 可访问版。 通过 user-agent 判断来访者，将 Baiduspider 定向到 http 页面，普通用户通过百度搜索引擎访问该页面时，通过 301 重定向至相应的 https 页面。如图所示，上图为百度收录的 http 版本，下图为用户点击后会自动跳入 https 版本。 http 版本不是只为首页制作，其它重要页面也需要制作 http 版本，且互相链接，切不要出现这种情况：首页 http 页面上的链接依然链向 https 页面，导致 Baiduspider 无法继续抓取——我们曾经遇到这种情况，导致整个网点我们只能收录一个首页。如下面这个做法就是错误的：http://www.abc.com/ 链向 https://www.adc.com/bbs/ 可以将部分不需要加密的内容，如资讯等使用二级域名承载。比如支付宝站点，核心加密内容放在 https 上，可以让 Baiduspider 直接抓取的内容都放置在二级域名上。 站点切换 https 不会对流量产生负面影响 问：对于 http 和 https，Baiduspider 会不会区别对待？ 答：不会区别对待。说得再全整专业一些：Baiduspider 在对于 http 和 https 站点的调度和解析方面没有任何区别。至于建索引库，年初的时候百度进行过升级，目前对 https 站点也是全力支持的。 问：Baiduspider 可以抓取 https 网站吗？站点 https 以后应该做些什么？ 答：在抓取方面，Baiduspider 完全支持 https 链接的抓取。站长需要注意的一点是要保证 http 链接到 https 链接良好的重定向（使用 301 或 302），这样百度可以做到无缝切换。另外可以使用链接提交工具提交 https 链接，尽快通知百度。 问：站点切换 https 以后，新产生的 https 页面，百度会将其视为新页面重新收录？重新计算排名？ 答：不是的，不涉及重新收录的问题。 问：https 以后可以使用改版工具向百度提交 http 与 https 的对应关系吗？ 答：http 站和 https 站会被百度视为同一个站点，不属于改版，不适用改版工具。 问：https 以后，排序有可能发生什么变化？ 答：在 rank 方面，现在对两种链接无区别对待，从安全性考虑，以后可能会优待 https 的链接。 禁止百度保留快照的代码：noarchive 很多站点出于隐私的考虑不希望百度保留快照，网上也在讨论如何禁止百度保留快照的方法。其实百度早已对此有过说明，但藏在一篇不起眼的文章中不引人注目，导致依然非常多的人不清楚该如何操作。上周恰好又有人问到这个问题，特意进行说明。 要防止所有搜索引擎显示您网站的快照，请将此元标记置入网页的 部分： 要允许其他搜索引擎显示快照，但仅防止百度显示，请使用以下标记： 注：此标记只是禁止百度显示该网页的快照，并不会影响网页建入索引，同时垃圾网页也不可能依靠此手段逃避百度的判罚。 "},"seo/friendly-solution.html":{"url":"seo/friendly-solution.html","title":"移动站点对百度友好全解","keywords":"","body":"移动站点对百度友好全解 如何布局您的 PC 站和移动站，并表达两者之间内容的对应关系 目前较流量的 PC 站与移动站配置方式有三种，百度站在搜索引擎角度将这三种分别称为跳转适配、代码适配和自适应，以下为这三种配置方式的名词解释及异同对比。 跳转适配：该方法会利用单独的网址向每种设备提供不同的代码。这种配置会尝试检测用户所使用的设备或 ua，然后使用 HTTP 重定向和 Vary HTTP 标头重定向到相应的页面。 代码适配：该方法使用相同的网址（不考虑用户所使用的设备），但会根据服务器对用户所用浏览器的了解（ua），针对不同设备类型生成不同版本的 HTML。 自适应：通过同一网址提供相同 HTML 代码的网站设计方法。该方法不考虑用户所使用的设备（pc、平板电脑、移动设备），但可以根据屏幕尺寸以不同方式呈现（即适应）显示屏。 PC、移动网址是否一致 PC、移动网页代码是否一致 跳转适配 否 否 代码适配 是 否 自适应 是 是 三种配置方式的分析 百度仅站在搜索引擎角度对跳转适配、代码适配、自适应这三种配置方式做了一些对比和分析，希望能够帮助站点选择更适合自己、性价比最优的方式来进行移动化。 跳转适配 代码适配 自适应 复杂程度 简单到中等。开发独立网站的速度可以非常快。小型企业可选用多种自动方案，以近乎实时的速度生成移动网站。 中到高，取决于网站的复杂程度和您需要创建的代码库数量。代码适配所需的开发时间可能较长，且要求服务器端编程 中。需要使用能随屏幕尺寸而变的流体网格从头开始创建。如果网站需求较简单，有许多开源模板可供选择。如要构建包含额外编程的复杂的自适应网站，所需的时间会比较长。 性能 中。图片和其他网站内容可轻松针对小屏幕优化，但网站重定向经常会导致延迟问题。 高。可以简化为只包含为相应设备优化的内容，以实现最佳性能。 高。无任何重定向，但需要有周全的计划才能实现最优的效果。数据量膨胀是最常见的错误。 维护需求 中到高。更新主网站后，还必须单独在移动网站上进行更新。 如果人工维护，将需要大量的资源。许多网站使用内容管理系统来避免这一问题，并自动在所有模板上发布内容。 低。创建后，更新内容会流向所有设备，维护工作量极低。 设备 可专门针对移动用户优化网站。 单独的文件和服务器端代码（会在向用户传递网页前在您的服务器上运行）可以提供依设备而定的体验。 所有设备上的用户体验保持一致（一些设备专属的选项可通过服务器端程序添加）。 是否支持扩展至新平台 不支持。这是智能手机专用的独立移动网站。新平台无法轻松集成到现有架构中。 支持。可轻松针对具体的设备（例如智能电视）创建模板，并通过同一个网址投放。 支持。使用指定的断点和流体网格，可轻松扩展到新平台和新设备。 "},"seo/website-optimization.html":{"url":"seo/website-optimization.html","title":"网站优化","keywords":"","body":"网站优化 知名站点优化注意事项 百度需要优质站点为搜索引擎数据库源源不断地输入物料，同时优质站点也需要从百度获得搜索引擎用户，并将这些搜索引擎用户转化为自己的用户。知名站点可以视为优质站点的一部分，是指已经有较高用户知名度的网站。那么，站点越是知名，就越应从长远考虑，以用户体验为重，积极、合理的进行网站优化，远离作弊和恶意 SEO 行为，建立与百度更加稳固的合作关系。 但我们经常可以遗憾地看到一些知名站点使用了不够合理的内容建设方式，比如：大量不同内容的页面均使用同一标题；通过图片的方式展现网页中的重要信息（新闻、联系电话等）；重要页面通过 flash 建设，未使用文字说明等。 类似的方式，都会使搜索引擎对网站内容的理解造成困难，最终影响网站在搜索引擎中的表现。我们建议您采取对搜索引擎友好的方式进行网站建设，具体内容可参考《百度搜索引擎优化指南》以及《百度搜索引擎网页质量白皮书》。 知名站点对搜索引擎，乃至整个互联网欢迎的影响通常是巨大的，若使用恶意的作弊行为，其产生的影响也就越坏。例如： 出售首页的友情链接。 大量采集其他网站的内容，原创内容比例极低，甚至大量采集网站主题无关的内容，或机器批量生成没有价值的页面。 网站被大量广告内容占领。 知名站点一旦有恶意的作弊行为，所产生的恶劣影响将是巨大的，有可能会受到搜索引擎更为严格的惩罚。知名站点更应从长远考虑，提升用户口碑，用高质量的内容赢得用户青睐。不应为了短期的流量，不顾用户的体验，做有损品牌形象的事情。 此外，知名站点也是各路作弊者的 “众矢之的”，容易被恶意利用。站点越知名，被利用的价值就越大。我们建议您建立防范意识，避免由于被作弊者利用引起的不必要风险。 您的站点是否足够安全？当网站被黑客攻击时，是否有足够的应对机制？ 由用户贡献内容的功能，是否做好了应对作弊的措施？ 对于不想让搜索引擎收录的内容，是否在 robot.txt 中清晰指明？ 百度索引量变化追查投诉方法 相信无论站点知名，都对百度索引量数据非常敏感，毕竟索引量（或称收录量）属于网站流量的基石，盯着百度站长平台提供的索引量工具查看数据应该是很多站长每天的例行工作。理论上说，索引量发生波动属于正常，但也不应掉以轻心。可以通过以下流程图来进行问题排查。 关于申诉，除了再一次播报申诉地址（http://zhanzhang.baidu.com/feedback）外，我们给各位站长提个醒，在撰写申诉内容时应该尽量将问题描述具体，引用网友的经验：“网站索引量异常，可以使用百度站长平台的索引量查询工具，一级一级的遍历一下自己网站的主要子域名或目录，以确定到底是哪个子域名或目录的索引量出现了异常。”“每个频道选取一些页面，在百度网页搜索中直接搜索这样页面的 URL，以定位被删除快照网页的最小范围。”“然后在投诉内容中明确给出 '病体' 的 URL，并附上相应的数据变动截图。” 这样才便于处理投诉的百度工作人员快速寻找问题症结。 另外需要注意的是，站长们往往只有在索引量下降时心惊肉跳一把，在百度内部人员看来，索引量无缘无故大幅增长更应该警惕，应该检查一下： 会不会是网站被黑客攻击后增加了大量垃圾网页 会不会是 Robost 协议出了问题，导致大批保密页面被百度抓取 大幅增加的 url 会不会占用有限的抓取配额，导致重要优质内容未被抓取 "}}